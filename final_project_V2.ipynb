{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# DATA 612 FINAL PROJECT\n",
    "Amazon Product Recommender Model Using Reviews\n",
    "\n",
    "* Farhod Ibragimov\n",
    "* Gillian McGovern"
   ],
   "id": "c807f2c41b508e9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Objective\n",
    "\n",
    "Create an offline Amazon product (specifically Amazon appliances) recommender model using user ratings and reviews."
   ],
   "id": "f588361f909749de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Sources\n",
    "\n",
    "Source: https://amazon-reviews-2023.github.io/\n",
    "\n",
    "This is a large-scale Amazon Reviews dataset, collected in 2023 by McAuley Lab, and it includes rich features such as:\n",
    "\n",
    "*   User Reviews (ratings, text, helpfulness votes, etc.);\n",
    "*   Item Metadata (descriptions, price, raw image, etc.);\n",
    "*   Links (user-item / bought together graphs).\n",
    "\n",
    "\n",
    "User review structure can be found [here](https://amazon-reviews-2023.github.io/#for-user-reviews) and item metadata structure can be found [here](https://amazon-reviews-2023.github.io/#for-item-metadata).\n",
    "\n",
    "We will be specifically looking at the Appliances category of products, which includes:\n",
    "\n",
    "* 1.8M Users\n",
    "* 94.3K Appliances\n",
    "* 2.1M Ratings/Reviews\n",
    "\n",
    "The original data is in JSON format."
   ],
   "id": "4a0244e82324cfa7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Read in the Data",
   "id": "1545f5796f665192"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T21:04:48.001858Z",
     "start_time": "2025-07-16T21:04:47.979868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import packages and declare global variables\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import floor, round, monotonically_increasing_id, col\n",
    "from pyspark.sql import SparkSession\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "\n",
    "\n",
    "# CONFIG\n",
    "REVIEW_PATH = r\"Appliances.jsonl\"\n",
    "META_PATH   = r\"meta_Appliances.jsonl\"\n",
    "OUT_DIR     = r\"output\"\n",
    "CHUNK_SIZE  = 200_000   # tune based on your machine's RAM\n",
    "SEED        = 42\n",
    "TRAIN_PARQUET_PATH = r\"output/train.parquet\"\n",
    "TEST_PARQUET_PATH = r\"output/test.parquet\"\n",
    "FULL_REVIEW_PATH = r\"output/full_review.parquet\"\n",
    "EMB_OUT_PATH       = r\"output/embeddings/item_embeddings.parquet\"\n",
    "SAMPLE_PER    = 5\n",
    "BATCH_SIZE    = 64\n",
    "\n",
    "# S3 variables\n",
    "BUCKET = os.getenv('S3_BUCKET', 'farhodibr')\n",
    "PREFIX = \"notebook-data\"\n",
    "S3_BASE = f\"s3://{BUCKET}/{PREFIX}\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "fs = s3fs.S3FileSystem()"
   ],
   "id": "16a7a930d2a86fed",
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "9c312cad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:46:39.216972Z",
     "start_time": "2025-07-16T20:46:39.214146Z"
    }
   },
   "source": [
    "# Write test, train, and valid parquet files locally within folder\n",
    "# Uncomment if files are needed\n",
    "\n",
    "#\n",
    "# os.makedirs(OUT_DIR, exist_ok=True)\n",
    "#\n",
    "# #LOAD METADATA\n",
    "# meta_pd = pd.read_json(\n",
    "#     META_PATH,\n",
    "#     lines=True\n",
    "# )[[\"parent_asin\", \"average_rating\", \"rating_number\"]]\n",
    "#\n",
    "# writers = {\"train\": None, \"valid\": None, \"test\": None}\n",
    "#\n",
    "# # STREAM, FILTER, SPLIT, AND WRITE\n",
    "# for chunk in pd.read_json(\n",
    "#     REVIEW_PATH,\n",
    "#     lines=True,\n",
    "#     chunksize=CHUNK_SIZE\n",
    "# ):\n",
    "#     # 1) keep needed cols + timestamp\n",
    "#     chunk = chunk[[\"user_id\", \"parent_asin\", \"rating\", \"timestamp\", \"text\"]]\n",
    "#\n",
    "#     # 2) filter to years 2021–2023\n",
    "#     dt = pd.to_datetime(chunk[\"timestamp\"], unit=\"ms\")\n",
    "#     mask_year = dt.dt.year.between(2021, 2023)\n",
    "#     chunk = chunk.loc[mask_year]\n",
    "#     if chunk.empty:\n",
    "#         continue\n",
    "#\n",
    "#     # 3) assign random float for splitting\n",
    "#     rng = np.random.RandomState(SEED)\n",
    "#     chunk[\"_rand\"] = rng.rand(len(chunk))\n",
    "#\n",
    "#     # 4) merge metadata\n",
    "#     chunk = chunk.merge(meta_pd, on=\"parent_asin\", how=\"left\")\n",
    "#\n",
    "#     # 5) define split masks\n",
    "#     masks = {\n",
    "#         \"train\": chunk[\"_rand\"] <  0.8,\n",
    "#         \"valid\": (chunk[\"_rand\"] >= 0.8) & (chunk[\"_rand\"] < 0.9),\n",
    "#         \"test\":  chunk[\"_rand\"] >= 0.9\n",
    "#     }\n",
    "#\n",
    "#     # 6) write each split to its Parquet\n",
    "#     for split, m in masks.items():\n",
    "#         sub = chunk.loc[m, [\n",
    "#             \"user_id\",\n",
    "#             \"parent_asin\",\n",
    "#             \"rating\",\n",
    "#             \"text\",\n",
    "#             \"average_rating\",\n",
    "#             \"rating_number\"\n",
    "#         ]]\n",
    "#         if sub.empty:\n",
    "#             continue\n",
    "#         tbl = pa.Table.from_pandas(sub, preserve_index=False)\n",
    "#         path = os.path.join(OUT_DIR, f\"{split}.parquet\")\n",
    "#         if writers[split] is None:\n",
    "#             writers[split] = pq.ParquetWriter(path, schema=tbl.schema)\n",
    "#         writers[split].write_table(tbl)\n",
    "#\n",
    "# # close Parquet writers\n",
    "# for w in writers.values():\n",
    "#     if w:\n",
    "#         w.close()\n",
    "#\n",
    "# print(\"Finished writing filtered splits to:\", OUT_DIR)\n",
    "# print(\"  •\", os.path.join(OUT_DIR, \"train.parquet\"))\n",
    "# print(\"  •\", os.path.join(OUT_DIR, \"valid.parquet\"))\n",
    "# print(\"  •\", os.path.join(OUT_DIR, \"test.parquet\"))\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "2aa20bc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:46:39.248352Z",
     "start_time": "2025-07-16T20:46:39.235592Z"
    }
   },
   "source": [
    "# for split in (\"train\", \"valid\", \"test\"):\n",
    "#     path = rf\"output/{split}.parquet\"\n",
    "#     pf = pq.ParquetFile(path)\n",
    "#     print(f\"{split.capitalize()} split: {pf.metadata.num_rows} reviews\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split: 616463 reviews\n",
      "Valid split: 77195 reviews\n",
      "Test split: 76002 reviews\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "0b113b5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:59:40.603398Z",
     "start_time": "2025-07-16T20:59:13.162151Z"
    }
   },
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CheckRatingRange\") \\\n",
    "    .config(\"spark.driver.memory\",\"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load just the rating column from the train split\n",
    "# train = spark.read.parquet(\n",
    "#     r\"output/train.parquet\"\n",
    "# )\n",
    "\n",
    "# Reading from s3\n",
    "train_pd = pd.read_parquet(f\"{S3_BASE}/{TRAIN_PARQUET_PATH}\")\n",
    "train = spark.createDataFrame(train_pd)\n",
    "\n",
    "# 1) Preview the data\n",
    "train.show(5)\n",
    "\n",
    "# 2) min & max\n",
    "train.selectExpr(\"min(rating) AS min_rating\", \"max(rating) AS max_rating\") \\\n",
    "     .show()\n",
    "\n",
    "# 3) basic summary (mean, stddev, etc.)\n",
    "train.describe(\"rating\").show()\n",
    "\n",
    "# 4) breakdown by rating value\n",
    "train.groupBy(\"rating\").count().orderBy(\"rating\").show(5, truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/16 16:59:29 WARN TaskSetManager: Stage 84 contains a task of very large size (19238 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------+--------------------+--------------+-------------+\n",
      "|             user_id|parent_asin|rating|                text|average_rating|rating_number|\n",
      "+--------------------+-----------+------+--------------------+--------------+-------------+\n",
      "|AHWWLSPCJMALVHDDV...| B07DD37QPZ|     5|Little on the thi...|           4.4|         3186|\n",
      "|AEUH4EH6XHROLT7UZ...| B099ZKQJHK|     5|After buying this...|           4.1|          506|\n",
      "|AHCV2CNCOCG6WECDR...| B001TH7H0O|     2|Not the best quality|           4.3|        11035|\n",
      "|AFUOYIZBU3MTBOLYK...| B085C6C7WH|     2|The company respo...|           3.8|          223|\n",
      "|AHPUT3ITXCHQJO7OM...| B09CBF2XCF|     4|Love little kitch...|           4.5|        15159|\n",
      "+--------------------+-----------+------+--------------------+--------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/16 16:59:31 WARN TaskSetManager: Stage 85 contains a task of very large size (19238 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|min_rating|max_rating|\n",
      "+----------+----------+\n",
      "|         1|         5|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/16 16:59:34 WARN TaskSetManager: Stage 88 contains a task of very large size (19238 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|           rating|\n",
      "+-------+-----------------+\n",
      "|  count|           616463|\n",
      "|   mean|4.079331606276451|\n",
      "| stddev|1.495287136111409|\n",
      "|    min|                1|\n",
      "|    max|                5|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/16 16:59:36 WARN TaskSetManager: Stage 91 contains a task of very large size (19238 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|rating|count |\n",
      "+------+------+\n",
      "|1     |92213 |\n",
      "|2     |26887 |\n",
      "|3     |32717 |\n",
      "|4     |52611 |\n",
      "|5     |412035|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BERT Content Based Recommender Model",
   "id": "f2e977b72120ab85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create BERT Embeddings",
   "id": "c5f23ed9d90f8cc2"
  },
  {
   "cell_type": "code",
   "id": "4680b3b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:46:46.197499Z",
     "start_time": "2025-07-16T20:46:46.192175Z"
    }
   },
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from pyspark.sql import SparkSession\n",
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "#\n",
    "# DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {DEVICE}\")\n",
    "#\n",
    "# os.makedirs(os.path.dirname(EMB_OUT), exist_ok=True)\n",
    "#\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"ItemBERTEmbeddings\") \\\n",
    "#     .config(\"spark.driver.memory\", \"16g\") \\\n",
    "#     .getOrCreate()\n",
    "#\n",
    "# df = spark.read.parquet(TRAIN_PARQUET_PATH).select(\"parent_asin\", \"text\")\n",
    "# pdf = df.toPandas()\n",
    "# spark.stop()\n",
    "#\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=DEVICE)\n",
    "#\n",
    "# writer = None\n",
    "# schema = None\n",
    "#\n",
    "# for pid, group in pdf.groupby(\"parent_asin\", sort=False):\n",
    "#     texts = group[\"text\"].sample(\n",
    "#         n=min(len(group), SAMPLE_PER),\n",
    "#         random_state=42\n",
    "#     ).tolist()\n",
    "#\n",
    "#     embs = model.encode(\n",
    "#         texts,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         show_progress_bar=False,\n",
    "#         convert_to_numpy=True\n",
    "#     )\n",
    "#     mean_emb = embs.mean(axis=0)\n",
    "#\n",
    "#     data = {\"parent_asin\": [pid]}\n",
    "#     for i, v in enumerate(mean_emb):\n",
    "#         data[f\"emb_{i}\"] = [float(v)]\n",
    "#     table = pa.Table.from_pydict(data)\n",
    "#\n",
    "#     if writer is None:\n",
    "#         schema = table.schema\n",
    "#         writer = pq.ParquetWriter(EMB_OUT, schema=schema)\n",
    "#\n",
    "#\n",
    "#     writer.write_table(table)\n",
    "#\n",
    "#\n",
    "# if writer:\n",
    "#     writer.close()\n",
    "#\n",
    "# print(\"Wrote item embeddings to:\", EMB_OUT)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create Content Based Model Using BERT Embeddings",
   "id": "2e5671e081f96169"
  },
  {
   "cell_type": "code",
   "id": "8529d4a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:46:46.230193Z",
     "start_time": "2025-07-16T20:46:46.224929Z"
    }
   },
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "#\n",
    "# # ─── CONFIG ──────────────────────────────────────────────────────────────────\n",
    "# TOP_K      = 5\n",
    "#\n",
    "# # ─── 1) Load embeddings and metadata ─────────────────────────────────────────\n",
    "# df_emb = pd.read_parquet(EMB_PATH)\n",
    "# df_meta = pd.read_json(META_PATH, lines=True)[[\"parent_asin\", \"title\"]]\n",
    "# df_meta = df_meta.rename(columns={\"parent_asin\": \"item_id\", \"title\": \"product_title\"})\n",
    "#\n",
    "# # ─── 2) Merge to get titles alongside embeddings ─────────────────────────────\n",
    "# df = df_emb.rename(columns={\"parent_asin\": \"item_id\"}).merge(df_meta, on=\"item_id\", how=\"left\")\n",
    "#\n",
    "# # ─── 3) Fit Nearest Neighbors on embedding vectors ────────────────────────────\n",
    "# X = df.filter(regex=\"^emb_\").values\n",
    "# item_ids = df[\"item_id\"].values\n",
    "# titles   = df[\"product_title\"].values\n",
    "#\n",
    "# nn = NearestNeighbors(n_neighbors=TOP_K+1, metric=\"cosine\")\n",
    "# nn.fit(X)\n",
    "#\n",
    "# # ─── 4) Recommendation function returning titles ─────────────────────────────\n",
    "# def recommend_titles(item_id: str, top_k: int = TOP_K):\n",
    "#     if item_id not in item_ids:\n",
    "#         raise ValueError(f\"Item ID {item_id} not found.\")\n",
    "#     idx = np.where(item_ids == item_id)[0][0]\n",
    "#     distances, indices = nn.kneighbors([X[idx]], n_neighbors=top_k+1)\n",
    "#     rec_idxs = indices[0][1:]\n",
    "#     return titles[rec_idxs].tolist()\n",
    "#\n",
    "# # ─── 5) Show for a sample item ───────────────────────────────────────────────\n",
    "# sample_id = item_ids[:5]\n",
    "# print(\"Sample item:\")\n",
    "# print(\" - ID:   \", sample_id)\n",
    "# print(\" - Title:\", titles[0])\n",
    "# print(\"\\nTop 5 similar items by title:\")\n",
    "# for rank, pt in enumerate(recommend_titles(sample_id), start=1):\n",
    "#     print(f\"{rank}. {pt}\")\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Collaborative Filtering Spark Model (ALS)",
   "id": "95e90b369a3b8560"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:46:46.256999Z",
     "start_time": "2025-07-16T20:46:46.253413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Can keep this commented out bc we already ran this\n",
    "# # Parquets should be present in GitHub repo\n",
    "\n",
    "# review_json_chunks = pd.read_json(REVIEW_PATH, lines=True, chunksize=CHUNK_SIZE)\n",
    "#\n",
    "# writer = None\n",
    "# schema = None\n",
    "# # Iterate through the chunks and process each DataFrame\n",
    "# all_dfs = []\n",
    "# for i, chunk_df in enumerate(review_json_chunks):\n",
    "#     print(f\"Processing Chunk {i+1}, Shape: {chunk_df.shape}\")\n",
    "#     chunk_df = chunk_df[[\"user_id\", \"parent_asin\", \"rating\"]]\n",
    "#\n",
    "#     table = pa.Table.from_pandas(chunk_df, preserve_index=False)\n",
    "#\n",
    "#     if writer is None:\n",
    "#         schema = table.schema\n",
    "#         writer = pq.ParquetWriter(r\"output/full_review.parquet\", schema=schema)\n",
    "#\n",
    "#\n",
    "#     writer.write_table(table)\n",
    "#\n",
    "#\n",
    "# if writer:\n",
    "#     writer.close()\n",
    "#\n",
    "# print(\"Completed writing review parquet file\")"
   ],
   "id": "9b89d2e5f32760f4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T21:06:51.942018Z",
     "start_time": "2025-07-16T21:06:16.663996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CheckRatingRange\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the training and test data\n",
    "# als_full_df = spark.read.parquet(r\"output/full_review.parquet\")\n",
    "\n",
    "# read parquets from s3\n",
    "als_full_pd = pd.read_parquet(f\"{S3_BASE}/{FULL_REVIEW_PATH}\")\n",
    "als_full_df = spark.createDataFrame(als_full_pd)\n",
    "als_full_df_cached = als_full_df.cache()\n",
    "del als_full_pd"
   ],
   "id": "c69b1345e48ac87e",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T21:07:29.656067Z",
     "start_time": "2025-07-16T21:07:07.229456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ALS data preparation - ALS good for sparse data\n",
    "\n",
    "users = als_full_df_cached.select(\"user_id\").distinct()\n",
    "# We want to prevent the same data/id to be used in different partitions, so just force use 1 partition for this\n",
    "users = users.coalesce(1)\n",
    "users = users.withColumn(\"userIntId\", monotonically_increasing_id()).persist() # Use persist to keep these values the same\n",
    "\n",
    "products = als_full_df_cached.select(\"parent_asin\").distinct()\n",
    "products = products.coalesce(1)\n",
    "products = products.withColumn(\"productIntId\", monotonically_increasing_id()).persist()\n",
    "\n",
    "als_df_int_ids = als_full_df_cached.join(users, \"user_id\", \"left\").join(products, \"parent_asin\", \"left\")\n",
    "display(als_df_int_ids.show(5))"
   ],
   "id": "27d0ee3375fa1fd8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/16 17:07:07 WARN TaskSetManager: Stage 0 contains a task of very large size (10053 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/16 17:07:12 WARN TaskSetManager: Stage 1 contains a task of very large size (10053 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/16 17:07:13 WARN TaskSetManager: Stage 2 contains a task of very large size (10053 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/16 17:07:16 WARN TaskSetManager: Stage 5 contains a task of very large size (10053 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/16 17:07:18 WARN TaskSetManager: Stage 8 contains a task of very large size (10053 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------+---------+------------+\n",
      "|parent_asin|             user_id|rating|userIntId|productIntId|\n",
      "+-----------+--------------------+------+---------+------------+\n",
      "| B08D6KFGW7|AEUJN55YZH6HCUFUP...|     5|     5187|       60769|\n",
      "| B08R896G15|AF4A4E4B53V37NKW3...|     1|   153629|       78293|\n",
      "| B07BGHC4TD|AFJFAWK2477YBZZRL...|     5|    21878|       77321|\n",
      "| B09YC8YCV6|AGD2MDSKRK6NY5EAH...|     5|   116233|       13620|\n",
      "| B07633SRDK|AGD2MDSKRK6NY5EAH...|     5|   116233|       74929|\n",
      "+-----------+--------------------+------+---------+------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T21:09:04.862955Z",
     "start_time": "2025-07-16T21:09:04.250314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the ratings into training and test data\n",
    "als_df_final = als_df_int_ids.select(col(\"userIntId\").alias(\"userId\"), col(\"productIntId\").alias(\"productId\"), col(\"rating\"), col('parent_asin'))\n",
    "als_df_final_cached = als_df_final.cache()\n",
    "\n",
    "(training_data, test_data) = als_df_final_cached.randomSplit([0.7, 0.3], seed=42)\n",
    "test_data_cached = test_data.cache()\n",
    "training_data_cached = training_data.cache()"
   ],
   "id": "5515030f5774796e",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T21:10:19.896258Z",
     "start_time": "2025-07-16T21:09:06.378698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create model without any hyperparameter tuning\n",
    "\n",
    "# Set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(userCol=\"userId\", itemCol=\"productId\", ratingCol=\"rating\", rank = 10, maxIter = 15, regParam = .1,\n",
    "          coldStartStrategy=\"drop\", nonnegative = True, implicitPrefs = False)\n",
    "\n",
    "# Fit the model to the training_data\n",
    "model = als.fit(training_data_cached)\n",
    "\n",
    "# Generate predictions on the test_data\n",
    "test_predictions = model.transform(test_data_cached)\n",
    "\n",
    "# Preview the predictions result\n",
    "display(test_predictions.show(10))\n",
    "\n",
    "spark.stop()"
   ],
   "id": "c32a0258059dc84f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/16 17:09:06 WARN TaskSetManager: Stage 17 contains a task of very large size (10053 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+-----------+----------+\n",
      "|userId|productId|rating|parent_asin|prediction|\n",
      "+------+---------+------+-----------+----------+\n",
      "|    28|     4748|     4| B0963ZXRM6|  3.393458|\n",
      "|    28|     5684|     5| B0B9Z7PKW5| 3.5533638|\n",
      "|    28|    70604|     4| B093FM7GJD|  2.520297|\n",
      "|   148|     3331|     5| B00DM8J15C| 3.2995844|\n",
      "|   148|    21264|     5| B00ECV2MRC| 2.8162053|\n",
      "|   155|    70608|     1| B0014DZ2YG| 3.4768877|\n",
      "|   183|    53764|     4| B01N0TQ0OH| 3.2370622|\n",
      "|   211|    10331|     5| B001ICYB2M|  4.537841|\n",
      "|   385|    36806|     4| B00T3JMVY2| 2.3790338|\n",
      "|   496|      139|     4| B098NG86D7| 2.1709495|\n",
      "+------+---------+------+-----------+----------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Tuning",
   "id": "21461dc88987178d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:47:24.606011Z",
     "start_time": "2025-07-16T20:03:45.313604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TO DO: hyperparameter tuning\n",
    "\n",
    "# # Hyperparameter Tuning\n",
    "#\n",
    "# # Use pyspark grid search\n",
    "# param_grid = ParamGridBuilder() \\\n",
    "#            .addGrid(als.rank, [10, 50, 75, 100]) \\\n",
    "#            .addGrid(als.maxIter, [10]) \\\n",
    "#            .addGrid(als.regParam, [.05, .1, .15]) \\\n",
    "#            .build()\n",
    "#\n",
    "# # Create RMSE evaluator\n",
    "# evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "#\n",
    "# # Use cross validation\n",
    "# cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5, collectSubModels=True)\n",
    "#\n",
    "# # Checkpoint the training data to truncate its lineage.\n",
    "# # This is a lazy operation, it will be triggered by the .fit() call.\n",
    "# training_data_chkp = training_data_cached.checkpoint()\n",
    "#\n",
    "# # Fit the cross validator on the CHECKPOINTED DataFrame.\n",
    "# model = cv.fit(training_data_chkp)\n",
    "#\n",
    "# # Best model\n",
    "# best_model = model.bestModel\n",
    "#\n",
    "# # Average RMSE for each model\n",
    "# avg_rmse_models = model.avgMetrics\n",
    "#\n",
    "# display(f\"{len(param_grid)} models tested\")"
   ],
   "id": "cf9a0fdcfe2507df",
   "outputs": [],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stl-recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
