{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67aa21d",
   "metadata": {},
   "source": [
    "# DATA 612 FINAL PROJECT\n",
    "Amazon Product Recommender Model Using Reviews\n",
    "\n",
    "* Farhod Ibragimov\n",
    "* Gillian McGovern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481659ac",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Create an offline Amazon product (specifically Amazon appliances) recommender model using user ratings and reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3beadc",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "Source: https://amazon-reviews-2023.github.io/\n",
    "\n",
    "This is a large-scale Amazon Reviews dataset, collected in 2023 by McAuley Lab, and it includes rich features such as:\n",
    "\n",
    "*   User Reviews (ratings, text, helpfulness votes, etc.);\n",
    "*   Item Metadata (descriptions, price, raw image, etc.);\n",
    "*   Links (user-item / bought together graphs).\n",
    "\n",
    "\n",
    "User review structure can be found [here](https://amazon-reviews-2023.github.io/#for-user-reviews) and item metadata structure can be found [here](https://amazon-reviews-2023.github.io/#for-item-metadata).\n",
    "\n",
    "We will be specifically looking at the Appliances category of products, which includes:\n",
    "\n",
    "* 1.8M Users\n",
    "* 94.3K Appliances\n",
    "* 2.1M Ratings/Reviews\n",
    "\n",
    "The original data is in JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1f381f",
   "metadata": {},
   "source": [
    "## Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "543af7cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T00:49:00.423627Z",
     "start_time": "2025-07-18T00:49:00.409681Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import torch\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql import SparkSession\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import floor, round, monotonically_increasing_id, col\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ],
   "outputs": [],
   "execution_count": 180
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c312cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if False:\n",
    "    # CONFIG \n",
    "    REVIEW_PATH = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\Appliances.jsonl\"\n",
    "    META_PATH   = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\meta_Appliances.jsonl\"\n",
    "    OUT_DIR     = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\"\n",
    "    CHUNK_SIZE  = 200_000   # tune based on your machine's RAM\n",
    "    SEED        = 42\n",
    "\n",
    "\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    #LOAD METADATA \n",
    "    meta_pd = pd.read_json(\n",
    "        r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\meta_Appliances.jsonl\",\n",
    "        lines=True\n",
    "    )[[\"parent_asin\", \"average_rating\", \"rating_number\"]]\n",
    "\n",
    "    writers = {\"train\": None, \"valid\": None, \"test\": None}\n",
    "\n",
    "    # STREAM, FILTER, SPLIT, AND WRITE\n",
    "    for chunk in pd.read_json(\n",
    "        r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\Appliances.jsonl\",\n",
    "        lines=True,\n",
    "        chunksize=CHUNK_SIZE\n",
    "    ):\n",
    "        # 1) keep needed cols + timestamp\n",
    "        chunk = chunk[[\"user_id\", \"parent_asin\", \"rating\", \"timestamp\", \"text\"]]\n",
    "\n",
    "        # 2) filter to years 2021‚Äì2023\n",
    "        dt = pd.to_datetime(chunk[\"timestamp\"], unit=\"ms\")\n",
    "        mask_year = dt.dt.year.between(2021, 2023)\n",
    "        chunk = chunk.loc[mask_year]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        # 3) assign random float for splitting\n",
    "        rng = np.random.RandomState(SEED)\n",
    "        chunk[\"_rand\"] = rng.rand(len(chunk))\n",
    "\n",
    "        # 4) merge metadata\n",
    "        chunk = chunk.merge(meta_pd, on=\"parent_asin\", how=\"left\")\n",
    "\n",
    "        # 5) define split masks\n",
    "        masks = {\n",
    "            \"train\": chunk[\"_rand\"] <  0.8,\n",
    "            \"valid\": (chunk[\"_rand\"] >= 0.8) & (chunk[\"_rand\"] < 0.9),\n",
    "            \"test\":  chunk[\"_rand\"] >= 0.9\n",
    "        }\n",
    "\n",
    "        # 6) write each split to its Parquet\n",
    "        for split, m in masks.items():\n",
    "            sub = chunk.loc[m, [\n",
    "                \"user_id\",\n",
    "                \"parent_asin\",\n",
    "                \"rating\",\n",
    "                \"text\",\n",
    "                \"average_rating\",\n",
    "                \"rating_number\"\n",
    "            ]]\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            tbl = pa.Table.from_pandas(sub, preserve_index=False)\n",
    "            path = os.path.join(OUT_DIR, f\"{split}.parquet\")\n",
    "            if writers[split] is None:\n",
    "                writers[split] = pq.ParquetWriter(path, schema=tbl.schema)\n",
    "            writers[split].write_table(tbl)\n",
    "\n",
    "    # close Parquet writers\n",
    "    for w in writers.values():\n",
    "        if w:\n",
    "            w.close()\n",
    "\n",
    "    print(\"Finished writing filtered splits to:\", OUT_DIR)\n",
    "    print(\"  ‚Ä¢\", os.path.join(OUT_DIR, \"train.parquet\"))\n",
    "    print(\"  ‚Ä¢\", os.path.join(OUT_DIR, \"valid.parquet\"))\n",
    "    print(\"  ‚Ä¢\", os.path.join(OUT_DIR, \"test.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa20bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:    \n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    for split in (\"train\", \"valid\", \"test\"):\n",
    "        path = rf\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\{split}.parquet\"\n",
    "        pf = pq.ParquetFile(path)\n",
    "        print(f\"{split.capitalize()} split: {pf.metadata.num_rows} reviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b113b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "if False:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"CheckRatingRange\") \\\n",
    "        .config(\"spark.driver.memory\",\"4g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Load just the rating column from the train split\n",
    "    train = spark.read.parquet(\n",
    "        r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\train.parquet\"\n",
    "    ).select(\"rating\")\n",
    "\n",
    "    # 1) min & max\n",
    "    train.selectExpr(\"min(rating) AS min_rating\", \"max(rating) AS max_rating\") \\\n",
    "        .show()\n",
    "\n",
    "    # 2) basic summary (mean, stddev, etc.)\n",
    "    train.describe(\"rating\").show()\n",
    "\n",
    "    # 3) breakdown by rating value\n",
    "    train.groupBy(\"rating\").count().orderBy(\"rating\").show(5, truncate=False)\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651ded8",
   "metadata": {},
   "source": [
    "## BERT Content Based Recommender Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73a532",
   "metadata": {},
   "source": [
    "### Create BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4680b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN THIS CELL UNLESS EMBEDDINGS ARE NEEDED\n",
    "if False:\n",
    "    import os\n",
    "    import torch\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    from pyspark.sql import SparkSession\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from huggingface_hub import hf_hub_download\n",
    "\n",
    "    HF_REPO      = \"glavvrach79/my-recsys-data\"\n",
    "    HF_SUBFOLDER = \"output/embeddings\"\n",
    "    FNAME        = \"item_embeddings.parquet\"\n",
    "\n",
    "    LOCAL_TRAIN  = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\train.parquet\"\n",
    "    EMB_OUT      = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\embeddings\\item_embeddings.parquet\"\n",
    "\n",
    "    DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SAMPLE_PER   = 5\n",
    "    BATCH_SIZE   = 64\n",
    "\n",
    "    os.makedirs(os.path.dirname(EMB_OUT), exist_ok=True)\n",
    "    print(f\"Using device: {DEVICE}\\n\")\n",
    "\n",
    "    def is_good_parquet(path: str) -> bool:\n",
    "        \"\"\"Try a light PyArrow read to confirm the file is a valid Parquet.\"\"\"\n",
    "        try:\n",
    "            pq.read_table(path, columns=[])  # only footer\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    downloaded = False\n",
    "\n",
    "    # if a local copy already exists & is valid, use it \n",
    "    if os.path.exists(EMB_OUT) and is_good_parquet(EMB_OUT):\n",
    "        print(f\"Found valid local embeddings ‚Üí {EMB_OUT}\\n\")\n",
    "        downloaded = True\n",
    "\n",
    "    # else try downloading from HF \n",
    "    if not downloaded:\n",
    "        try:\n",
    "            hf_path = hf_hub_download(\n",
    "                repo_id=HF_REPO,\n",
    "                repo_type=\"dataset\",\n",
    "                subfolder=HF_SUBFOLDER,\n",
    "                filename=FNAME,\n",
    "            )\n",
    "            print(f\"üì¶ Downloaded HF embeddings ‚Üí {hf_path}\")\n",
    "            if is_good_parquet(hf_path):\n",
    "                print(\"HF parquet loads OK. Using HF version.\\n\")\n",
    "                EMB_OUT = hf_path\n",
    "                downloaded = True\n",
    "            else:\n",
    "                print(\"HF parquet is corrupt‚Äîwill recompute locally.\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch from HF ({e!r}); will compute locally.\\n\")\n",
    "\n",
    "    # if still not obtained, recompute locally \n",
    "    if not downloaded:\n",
    "        print(\"‚ñ∂Ô∏è Computing embeddings locally‚Ä¶\\n\")\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"ComputeItemEmbeddings\") \\\n",
    "            .config(\"spark.driver.memory\",\"16g\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        df = spark.read.parquet(LOCAL_TRAIN).select(\"parent_asin\", \"text\")\n",
    "        pdf = df.toPandas()\n",
    "        spark.stop()\n",
    "\n",
    "        model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=DEVICE)\n",
    "        writer = None\n",
    "\n",
    "        for pid, group in pdf.groupby(\"parent_asin\", sort=False):\n",
    "            samples = group[\"text\"].sample(\n",
    "                n=min(len(group), SAMPLE_PER),\n",
    "                random_state=42\n",
    "            ).tolist()\n",
    "\n",
    "            emb_batch = model.encode(\n",
    "                samples,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                show_progress_bar=False,\n",
    "                convert_to_numpy=True\n",
    "            )\n",
    "            mean_emb = emb_batch.mean(axis=0)\n",
    "\n",
    "            data = {\"parent_asin\": [pid]}\n",
    "            for i, val in enumerate(mean_emb):\n",
    "                data[f\"emb_{i}\"] = [float(val)]\n",
    "            table = pa.Table.from_pydict(data)\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(EMB_OUT, schema=table.schema)\n",
    "            writer.write_table(table)\n",
    "\n",
    "        if writer:\n",
    "            writer.close()\n",
    "\n",
    "        print(f\"‚úÖ Wrote item embeddings locally ‚Üí {EMB_OUT}\\n\")\n",
    "\n",
    "    # preview final \n",
    "    print(f\" Final embeddings path ‚Üí {EMB_OUT}\\n\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PreviewEmbeddings\") \\\n",
    "        .config(\"spark.driver.memory\",\"4g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    def try_spark_read(path: str):\n",
    "        try:\n",
    "            return spark.read.parquet(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Spark read failed on `{path}`:\\n   {e}\")\n",
    "            return None\n",
    "\n",
    "    emb_df = try_spark_read(EMB_OUT)\n",
    "\n",
    "    # ‚îÄ‚îÄ If Spark failed on the HF‚Äêcached file, fall back to pandas ‚Üí Spark ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    if emb_df is None:\n",
    "        import pandas as pd\n",
    "        print(\"Falling back to pandas.read_parquet ‚Ä¶\")\n",
    "        pdf = pd.read_parquet(EMB_OUT)\n",
    "        emb_df = spark.createDataFrame(pdf)\n",
    "\n",
    "    emb_df.show(5, truncate=50)\n",
    "    print(\"Total items:\", emb_df.count())\n",
    "    spark.stop()\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T21:20:24.603591Z",
     "start_time": "2025-07-17T21:20:06.410596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_meta = (\n",
    "        pd.read_json(META_PATH, lines=True)\n",
    "    )\n",
    "df_meta.head()"
   ],
   "id": "3edd8ea41338fc48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              main_category                                              title  average_rating  rating_number                                           features                                        description  price                                             images                                             videos      store                                         categories                                            details parent_asin  bought_together subtitle author\n",
       "0   Industrial & Scientific  ROVSUN Ice Maker Machine Countertop, Make 44lb...            3.70             61  [„ÄêQuick Ice Making„ÄëThis countertop ice machine...                                                 []    NaN  [{'thumb': 'https://m.media-amazon.com/images/...  [{'title': 'Our Point of View on the Euhomy Ic...     ROVSUN  [Appliances, Refrigerators, Freezers & Ice Mak...  {'Brand': 'ROVSUN', 'Model Name': 'ICM-2005', ...  B08Z743RRD              NaN      NaN    NaN\n",
       "1  Tools & Home Improvement  HANSGO Egg Holder for Refrigerator, Deviled Eg...            4.20             75  [Plastic, Practical Kitchen Storage - Our egg ...                                                 []    NaN  [{'thumb': 'https://m.media-amazon.com/images/...  [{'title': '10 Eggs Egg Holder for Refrigerato...     HANSGO  [Appliances, Parts & Accessories, Refrigerator...  {'Manufacturer': 'HANSGO', 'Part Number': 'HAN...  B097BQDGHJ              NaN      NaN    NaN\n",
       "2  Tools & Home Improvement  Clothes Dryer Drum Slide, General Electric, Ho...            3.50             18                                                 []  [Brand new dryer drum slide, replaces General ...    NaN  [{'thumb': 'https://m.media-amazon.com/images/...                                                 []         GE                  [Appliances, Parts & Accessories]  {'Manufacturer': 'RPI', 'Part Number': 'WE1M33...  B00IN9AGAE              NaN      NaN    NaN\n",
       "3  Tools & Home Improvement  154567702 Dishwasher Lower Wash Arm Assembly f...            4.50             26  [MODEL NUMBER:154567702 Dishwasher Lower Wash ...  [MODEL NUMBER:154567702 Dishwasher Lower Wash ...    NaN  [{'thumb': 'https://m.media-amazon.com/images/...                                                 []    folosem  [Appliances, Parts & Accessories, Dryer Parts ...  {'Manufacturer': 'folosem', 'Part Number': '15...  B0C7K98JZS              NaN      NaN    NaN\n",
       "4  Tools & Home Improvement                        Whirlpool W10918546 Igniter            3.80             12          [This is a Genuine OEM Replacement Part.]                                [Whirlpool Igniter]  25.07  [{'thumb': 'https://m.media-amazon.com/images/...                                                 []  Whirlpool                  [Appliances, Parts & Accessories]  {'Manufacturer': 'Whirlpool', 'Part Number': '...  B07QZHQTVJ              NaN      NaN    NaN"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>title</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>images</th>\n",
       "      <th>videos</th>\n",
       "      <th>store</th>\n",
       "      <th>categories</th>\n",
       "      <th>details</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>bought_together</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Industrial &amp; Scientific</td>\n",
       "      <td>ROVSUN Ice Maker Machine Countertop, Make 44lb...</td>\n",
       "      <td>3.70</td>\n",
       "      <td>61</td>\n",
       "      <td>[„ÄêQuick Ice Making„ÄëThis countertop ice machine...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[{'title': 'Our Point of View on the Euhomy Ic...</td>\n",
       "      <td>ROVSUN</td>\n",
       "      <td>[Appliances, Refrigerators, Freezers &amp; Ice Mak...</td>\n",
       "      <td>{'Brand': 'ROVSUN', 'Model Name': 'ICM-2005', ...</td>\n",
       "      <td>B08Z743RRD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tools &amp; Home Improvement</td>\n",
       "      <td>HANSGO Egg Holder for Refrigerator, Deviled Eg...</td>\n",
       "      <td>4.20</td>\n",
       "      <td>75</td>\n",
       "      <td>[Plastic, Practical Kitchen Storage - Our egg ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[{'title': '10 Eggs Egg Holder for Refrigerato...</td>\n",
       "      <td>HANSGO</td>\n",
       "      <td>[Appliances, Parts &amp; Accessories, Refrigerator...</td>\n",
       "      <td>{'Manufacturer': 'HANSGO', 'Part Number': 'HAN...</td>\n",
       "      <td>B097BQDGHJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tools &amp; Home Improvement</td>\n",
       "      <td>Clothes Dryer Drum Slide, General Electric, Ho...</td>\n",
       "      <td>3.50</td>\n",
       "      <td>18</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Brand new dryer drum slide, replaces General ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>GE</td>\n",
       "      <td>[Appliances, Parts &amp; Accessories]</td>\n",
       "      <td>{'Manufacturer': 'RPI', 'Part Number': 'WE1M33...</td>\n",
       "      <td>B00IN9AGAE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tools &amp; Home Improvement</td>\n",
       "      <td>154567702 Dishwasher Lower Wash Arm Assembly f...</td>\n",
       "      <td>4.50</td>\n",
       "      <td>26</td>\n",
       "      <td>[MODEL NUMBER:154567702 Dishwasher Lower Wash ...</td>\n",
       "      <td>[MODEL NUMBER:154567702 Dishwasher Lower Wash ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>folosem</td>\n",
       "      <td>[Appliances, Parts &amp; Accessories, Dryer Parts ...</td>\n",
       "      <td>{'Manufacturer': 'folosem', 'Part Number': '15...</td>\n",
       "      <td>B0C7K98JZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tools &amp; Home Improvement</td>\n",
       "      <td>Whirlpool W10918546 Igniter</td>\n",
       "      <td>3.80</td>\n",
       "      <td>12</td>\n",
       "      <td>[This is a Genuine OEM Replacement Part.]</td>\n",
       "      <td>[Whirlpool Igniter]</td>\n",
       "      <td>25.07</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Whirlpool</td>\n",
       "      <td>[Appliances, Parts &amp; Accessories]</td>\n",
       "      <td>{'Manufacturer': 'Whirlpool', 'Part Number': '...</td>\n",
       "      <td>B07QZHQTVJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 169
  },
  {
   "cell_type": "code",
   "id": "8529d4a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T15:24:08.694740Z",
     "start_time": "2025-07-17T15:24:08.384129Z"
    }
   },
   "source": [
    "# CONFIG \n",
    "HF_REPO      = \"glavvrach79/my-recsys-data\"\n",
    "HF_SUBFOLDER_KNN = \"output\"\n",
    "HF_SUBFOLDER = \"output/embeddings\"\n",
    "FNAME        = \"item_embeddings.parquet\"\n",
    "TRAIN_NAME   = \"train.parquet\"\n",
    "TEST_NAME  = \"test.parquet\"\n",
    "VALID_NAME = \"valid.parquet\"\n",
    "FULL_REVIEW_NAME = \"full_review.parquet\"\n",
    "KNN_NAME = \"knn_cache.parquet\"\n",
    "\n",
    "META_PATH = os.getenv('META_PATH', \"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\meta_Appliances.jsonl\")\n",
    "\n",
    "TOP_K        = 5\n",
    "OUT_PATH = os.getenv('OUT_PATH', \"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\knn_cache.parquet\")\n",
    "\n",
    "#os.makedirs(os.path.dirname(LOCAL_OUT), exist_ok=True)\n",
    "\n",
    "def load_or_build_cache():\n",
    "    # try HF download + read\n",
    "    try:\n",
    "        hf_cache = hf_hub_download(\n",
    "            repo_id=HF_REPO,\n",
    "            repo_type=\"dataset\",\n",
    "            subfolder=HF_SUBFOLDER_KNN,\n",
    "            filename=KNN_NAME,\n",
    "        )\n",
    "        print(\"Downloaded KNN cache from HF:\", hf_cache)\n",
    "        cache = pd.read_parquet(hf_cache)\n",
    "        print(\"Loaded cache from HF!\")\n",
    "        return cache\n",
    "    except Exception as e:\n",
    "        print(f\"HF cache unavailable ({e}); building locally‚Ä¶\")\n",
    "\n",
    "    \n",
    "    # download embeddings\n",
    "    hf_emb = hf_hub_download(\n",
    "        repo_id=HF_REPO,\n",
    "        repo_type=\"dataset\",\n",
    "        subfolder=HF_SUBFOLDER,\n",
    "        filename=FNAME,\n",
    "    )\n",
    "    df_emb = pd.read_parquet(hf_emb)\n",
    "\n",
    "    # metadata\n",
    "    df_meta = (\n",
    "        pd.read_json(f'r{META_PATH}', lines=True)\n",
    "          .loc[:, [\"parent_asin\", \"title\"]]\n",
    "          .rename(columns={\"parent_asin\":\"item_id\", \"title\":\"product_title\"})\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        df_emb.rename(columns={\"parent_asin\":\"item_id\"})\n",
    "              .merge(df_meta, on=\"item_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    X        = df.filter(regex=\"^emb_\").values\n",
    "    item_ids = df[\"item_id\"].to_numpy()\n",
    "    titles   = df[\"product_title\"].to_numpy()\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=TOP_K+1, metric=\"cosine\")\n",
    "    nn.fit(X)\n",
    "    distances, indices = nn.kneighbors(X)\n",
    "\n",
    "    title_map = df.set_index(\"item_id\")[\"product_title\"]\n",
    "\n",
    "    cache = pd.DataFrame({\n",
    "        \"item_id\":        np.repeat(item_ids, TOP_K),\n",
    "        \"item_title\":     np.repeat(title_map[item_ids].values, TOP_K),      # ‚Üê add this\n",
    "        \"rank\":           np.tile(np.arange(1, TOP_K+1), len(item_ids)),\n",
    "        \"neighbor_id\":    item_ids[indices[:, 1:]].ravel(),\n",
    "        \"neighbor_title\": titles[indices[:, 1:]].ravel(),\n",
    "        \"distance\":       distances[:, 1:].ravel(),\n",
    "    })\n",
    "    cache.to_parquet(OUT_PATH, index=False)\n",
    "    print(f\"file saved to {OUT_PATH}\")\n",
    "    \n",
    "    return cache\n",
    "\n",
    "\n",
    "knn_cache = load_or_build_cache()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded KNN cache from HF: /Users/gillianmcgovern/.cache/huggingface/hub/datasets--glavvrach79--my-recsys-data/snapshots/216e868cbbb098622e859a3b1275a215acc20122/output/knn_cache.parquet\n",
      "Loaded cache from HF!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "6189dd02",
   "metadata": {},
   "source": [
    "### Create Content Based Model Using BERT Embeddings"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T21:15:03.903789Z",
     "start_time": "2025-07-17T21:15:03.856577Z"
    }
   },
   "cell_type": "code",
   "source": "meta.head()",
   "id": "5bde09e8584dd6e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id\n",
       "B08Z743RRD    ROVSUN Ice Maker Machine Countertop, Make 44lb...\n",
       "B097BQDGHJ    HANSGO Egg Holder for Refrigerator, Deviled Eg...\n",
       "B00IN9AGAE    Clothes Dryer Drum Slide, General Electric, Ho...\n",
       "B0C7K98JZS    154567702 Dishwasher Lower Wash Arm Assembly f...\n",
       "B07QZHQTVJ                          Whirlpool W10918546 Igniter\n",
       "Name: product_title, dtype: object"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 168
  },
  {
   "cell_type": "code",
   "id": "bd0d8a0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T20:48:24.758791Z",
     "start_time": "2025-07-17T20:48:15.605886Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "CACHE = knn_cache\n",
    "\n",
    "# load the meta table so we can lookup item titles\n",
    "meta = (\n",
    "    pd.read_json(META_PATH, lines=True)\n",
    "      .rename(columns={\"parent_asin\":\"item_id\", \"title\":\"product_title\"})\n",
    "      .set_index(\"item_id\")[\"product_title\"]\n",
    ")\n",
    "\n",
    "def predict_using_bert(item_id):\n",
    "    # pull its top‚ÄëK neighbors from the cache\n",
    "    neighbors = (\n",
    "        CACHE[CACHE[\"item_id\"] == item_id]\n",
    "            .sort_values(\"rank\")\n",
    "            .loc[:, [\"neighbor_id\",\"neighbor_title\",\"distance\"]]\n",
    "    )\n",
    "    return neighbors\n",
    "\n",
    "random_id = CACHE[\"item_id\"].drop_duplicates().sample(1, random_state=42).iloc[0]\n",
    "random_title = meta[random_id]\n",
    "neighbors = predict_using_bert(random_id)\n",
    "\n",
    "print(f\"üé≤ Randomly selected: {random_id} ‚Üí {random_title}\\n\")\n",
    "print(\"Top similar items:\")\n",
    "print(neighbors.to_string(index=False, float_format=\"{:.4f}\".format, justify='left', max_colwidth=150))\n",
    "\n",
    "neighbors.to_parquet('BERT_similar_items.parquet', engine='pyarrow', compression='snappy')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Randomly selected: B07BTQKG5J ‚Üí [UPGRADED] DC47-00019A Samsung Dryer Heating Element & DC96-00887A Thermal Fuse & DC47-00018A Thermostat COMPLETE Dryer Repair Kit Replacement by BlueStars - Exact Fit For Samsung & Kenmore Dryers\n",
      "\n",
      "Top similar items:\n",
      "neighbor_id neighbor_title                                                                                                                                          distance\n",
      "B0B8MFHWT2  279973 & 3392519 & 8577274 Dryer Thermal Cut-Off Fuse Kit with Thermistor Control and Thermal Fuse by Blutoget -Compatible for Whirlpool, Ken-more,... 0.2482   \n",
      "B09TT3WN9H              279838 Heating Element for Kenmore Dryer Compatible Whirlpool Wed4815ew1 Heating Elements Roper Dryer Parts 500 600 70 80 Series Model 110 0.2597   \n",
      "B078RF452M  MAYITOP Compatible Dryer Heating Element for Samsung DV42H5200EP/A3-0000, Samsung DV361EWBEWR/A3-0001, Samsung DV219AEB/XAA-0000, Samsung DV393ETPA... 0.2622   \n",
      "B082PJ9BWH           137114000 Dryer Heating Element Kit,137032600 Thermal Limiter and High Limit Thermostat 3204267 Combo Set,Replacement for AP4456656 PS2367792 0.2631   \n",
      "B0BWCTGXFT  (2023 Update) 3392519 Dryer Thermal Fuse Replacement Part by BlueStars - Kenmore Dryer Thermal Fuse Exact Fit for Whirlpool Kenmore - Replaces AP60... 0.2726   \n"
     ]
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T00:35:15.007843Z",
     "start_time": "2025-07-18T00:35:14.915723Z"
    }
   },
   "cell_type": "code",
   "source": "neighbors.shape",
   "id": "32fe3c2c203723f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 170
  },
  {
   "cell_type": "markdown",
   "id": "2ca8b0ec",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Spark Model (ALS)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create Spark Session",
   "id": "63ff804adeec4ec"
  },
  {
   "cell_type": "code",
   "id": "613aa08d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T00:51:17.508772Z",
     "start_time": "2025-07-18T00:51:14.047876Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from huggingface_hub import hf_hub_download\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\").setAppName(\"SparkALS\")\n",
    "conf.set(\"spark.executor.memory\", \"16g\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark.sparkContext.setCheckpointDir(\"checkpoint_dir_als\")\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"ALSPrep\") \\\n",
    "#     .config(\"spark.driver.memory\", \"16g\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# download the reviews parquet from HF\n",
    "hf_path = hf_hub_download(\n",
    "    repo_id=\"glavvrach79/my-recsys-data\",\n",
    "    repo_type=\"dataset\",\n",
    "    filename=\"full_review.parquet\",\n",
    ")\n",
    "\n",
    "# read it directly with spark.read.parquet\n",
    "als_full_df = spark.read.parquet(hf_path).cache()\n",
    "\n",
    "#sanity‚Äêcheck the schema and small sample\n",
    "als_full_df.printSchema()\n",
    "als_full_df.limit(5).show()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/17 20:51:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------+\n",
      "|             user_id|parent_asin|rating|\n",
      "+--------------------+-----------+------+\n",
      "|AGKHLEW2SOWHNMFQI...| B01N0TQ0OH|     5|\n",
      "|AHWWLSPCJMALVHDDV...| B07DD37QPZ|     5|\n",
      "|AHZIJGKEWRTAEOZ67...| B082W3Z9YK|     5|\n",
      "|AFGUPTDFAWOHHL4LZ...| B078W2BJY8|     5|\n",
      "|AELFJFAXQERUSMTXJ...| B08C9LPCQV|     5|\n",
      "+--------------------+-----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 183
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T00:52:04.776616Z",
     "start_time": "2025-07-18T00:51:32.557671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "META_PATH = r\"data/meta_Appliances.jsonl\"\n",
    "meta = (\n",
    "    pd.read_json(META_PATH, lines=True)\n",
    ")\n",
    "meta = meta[['parent_asin', 'title']]\n",
    "meta = spark.createDataFrame(meta)"
   ],
   "id": "dc9778e7153e6523",
   "outputs": [],
   "execution_count": 184
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set Up Data for Spark ALS",
   "id": "a4f58541395f39da"
  },
  {
   "cell_type": "code",
   "id": "d1f9093e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T00:52:29.962836Z",
     "start_time": "2025-07-18T00:52:05.906545Z"
    }
   },
   "source": [
    "# distinct users + assign stable integer IDs\n",
    "users = (als_full_df\n",
    "         .select(\"user_id\")\n",
    "         .distinct()\n",
    "         .coalesce(1)\n",
    "         .withColumn(\"userIntId\", monotonically_increasing_id())\n",
    "         .cache())\n",
    "\n",
    "# distinct products + assign stable integer IDs\n",
    "products = (als_full_df\n",
    "            .select(\"parent_asin\")\n",
    "            .distinct()\n",
    "            .coalesce(1)\n",
    "            .withColumn(\"productIntId\", monotonically_increasing_id())\n",
    "            .cache())\n",
    "\n",
    "# join them back\n",
    "als_df_int_ids = (als_full_df\n",
    "                  .join(users,    \"user_id\",     \"left\")\n",
    "                  .join(products, \"parent_asin\", \"left\"))\n",
    "\n",
    "als_df_int_ids_cached = als_df_int_ids.cache()\n",
    "als_df_int_ids_cached = als_df_int_ids_cached.join(meta.select(col(\"parent_asin\"), col(\"title\")), on=['parent_asin'])\n",
    "\n",
    "# show a small sample\n",
    "als_df_int_ids_cached.limit(5).show()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/17 20:52:21 WARN TaskSetManager: Stage 19 contains a task of very large size (1361 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------+---------+------------+--------------------+\n",
      "|parent_asin|             user_id|rating|userIntId|productIntId|               title|\n",
      "+-----------+--------------------+------+---------+------------+--------------------+\n",
      "| B00002N7HY|AF7LBPZDD75YWNVPX...|     5|   115273|        4824|Leviton 5050 50 A...|\n",
      "| B00002N7HY|AEHB3PLJAATO6456H...|     5|  1063587|        4824|Leviton 5050 50 A...|\n",
      "| B00002N7HY|AEVQF4T2AFLPYBO25...|     5|  1092242|        4824|Leviton 5050 50 A...|\n",
      "| B00002N7HY|AF7B74NC2WNXBHEVA...|     5|  1362929|        4824|Leviton 5050 50 A...|\n",
      "| B00002N7HY|AE25KDKKEX3QMWVQ3...|     5|  1434114|        4824|Leviton 5050 50 A...|\n",
      "+-----------+--------------------+------+---------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 185
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split the Data for Spark ALS",
   "id": "743de962f33c9c6f"
  },
  {
   "cell_type": "code",
   "id": "492ec6ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T00:52:41.570203Z",
     "start_time": "2025-07-18T00:52:41.448851Z"
    }
   },
   "source": [
    "# Split the ratings into training and test data\n",
    "als_df_final = als_df_int_ids_cached.select(col(\"userIntId\").alias(\"userId\"), col(\"productIntId\").alias(\"productId\"), col(\"rating\"), col('title'), col('parent_asin'))\n",
    "als_df_final_cached = als_df_final.cache()\n",
    "\n",
    "(training_data, test_data) = als_df_final_cached.randomSplit([0.7, 0.3], seed=42)\n",
    "test_data_cached = test_data.cache()\n",
    "training_data_cached = training_data.cache()"
   ],
   "outputs": [],
   "execution_count": 186
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create Simple ALS Model (No Tuning)",
   "id": "e0e55c732f81d172"
  },
  {
   "cell_type": "code",
   "id": "ca3210e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T00:53:34.044883Z",
     "start_time": "2025-07-18T00:52:44.225671Z"
    }
   },
   "source": [
    "# Create model without any hyperparameter tuning\n",
    "\n",
    "# Set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(userCol=\"userId\", itemCol=\"productId\", ratingCol=\"rating\", rank = 10, maxIter = 15, regParam = .1,\n",
    "          coldStartStrategy=\"drop\", nonnegative = True, implicitPrefs = False)\n",
    "\n",
    "# Fit the model to the training_data\n",
    "model = als.fit(training_data_cached)\n",
    "\n",
    "# Generate predictions on the test_data\n",
    "test_predictions = model.transform(test_data_cached)\n",
    "test_predictions_cached = test_predictions.cache()\n",
    "\n",
    "# preview only the first 10 rows\n",
    "test_predictions_cached.limit(10).show(truncate=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/17 20:52:44 WARN TaskSetManager: Stage 30 contains a task of very large size (1361 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 235:==============================================>      (177 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+----------+\n",
      "|userId|productId|rating|title                                                                                                                                                                                                  |parent_asin|prediction|\n",
      "+------+---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+----------+\n",
      "|471   |38208    |4     |Waterdrop DA29-00020B NSF 53&42 Certified Refrigerator Water Filter, Replacement for Samsung DA29-00020B, DA29-00020A, HAF-CIN/EXP, 46-9101, WDS-F27, 1 Filter                                         |B0BVFQY9HH |3.7422144 |\n",
      "|4101  |6        |1     |Panda 2.6 cu.ft Compact Portable Laundry Dryer, 8.8 lbs capacity Intelligent Humidity Sensor dyer, White                                                                                               |B086WMG49L |0.79741204|\n",
      "|5518  |397      |4     |Imperial 28328 Combination Gas Valve Knob for Filter                                                                                                                                                   |B01858NQ9A |2.0289495 |\n",
      "|9465  |59359    |1     |Frigidaire WF3CB Puresource3 Refrigerator Water Filter , White, 1 Count (Pack of 1)                                                                                                                    |B0045LLC7K |2.891911  |\n",
      "|11033 |77787    |5     |Kuuk Fridge Egg Tray - Holder Container Box for 12 Large Eggs with Lid                                                                                                                                 |B01GO54TT2 |3.163967  |\n",
      "|11748 |44918    |5     |Honeywell HAC504 Replacement Humidifier Filter                                                                                                                                                         |B0000WM2IG |2.8261514 |\n",
      "|21220 |80745    |3     |Clothes Dryer Drum Slide, General Electric, Hotpoint, WE1M333, WE1M504                                                                                                                                 |B00IN9AGAE |3.9691875 |\n",
      "|28124 |90956    |5     |GE Profile Opal | Countertop Nugget Ice Maker with Side Tank | Portable Ice Machine with Bluetooth Connectivity | Smart Home Kitchen Essentials | Stainless Steel Finish | Up to 24 lbs. of Ice Per Day|B07YF9SGBW |0.71183026|\n",
      "|28577 |70598    |5     |Frigidaire 240338001 Door Bin for Refrigerator                                                                                                                                                         |B00545AUUG |3.3306441 |\n",
      "|28759 |12731    |5     |Whirlpool 53-0918 Lint                                                                                                                                                                                 |B00A8O06YS |2.0856533 |\n",
      "+------+---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 187
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Hyperparameter Tuning",
   "id": "26241f5005087cc5"
  },
  {
   "cell_type": "code",
   "id": "dfcf326d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T01:00:26.511111Z",
     "start_time": "2025-07-18T00:54:00.608949Z"
    }
   },
   "source": [
    "# Takes a while with current data\n",
    "# # Hyperparameter tuning\n",
    "#\n",
    "# # Use pyspark grid search\n",
    "# param_grid = ParamGridBuilder() \\\n",
    "#            .addGrid(als.rank, [10, 50, 75, 100]) \\\n",
    "#            .addGrid(als.maxIter, [10]) \\\n",
    "#            .addGrid(als.regParam, [.05, .1, .15]) \\\n",
    "#            .build()\n",
    "#\n",
    "# # Create RMSE evaluator\n",
    "# evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "#\n",
    "# # Use cross validation\n",
    "# cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5, collectSubModels=True)\n",
    "#\n",
    "# # Checkpoint the training data to truncate its lineage.\n",
    "# # This is a lazy operation, it will be triggered by the .fit() call.\n",
    "# training_data_chkp = training_data_cached.checkpoint()\n",
    "#\n",
    "# # Fit the cross validator on the CHECKPOINTED DataFrame.\n",
    "# model = cv.fit(training_data_chkp)\n",
    "#\n",
    "# # Best model\n",
    "# best_model = model.bestModel\n",
    "#\n",
    "# # Average RMSE for each model\n",
    "# avg_rmse_models = model.avgMetrics\n",
    "#\n",
    "# display(f\"{len(param_grid)} models tested\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1081:>                                                      (0 + 8) / 10]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py:853\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    852\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 853\u001B[0m     item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_items\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpopleft\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m:\n",
      "\u001B[0;31mIndexError\u001B[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[188], line 21\u001B[0m\n\u001B[1;32m     18\u001B[0m training_data_chkp \u001B[38;5;241m=\u001B[39m training_data_cached\u001B[38;5;241m.\u001B[39mcheckpoint()\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Fit the cross validator on the CHECKPOINTED DataFrame.\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_data_chkp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Best model\u001B[39;00m\n\u001B[1;32m     24\u001B[0m best_model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mbestModel\n",
      "File \u001B[0;32m~/PyCharmMiscProject/.venv/lib/python3.9/site-packages/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    208\u001B[0m     )\n",
      "File \u001B[0;32m~/PyCharmMiscProject/.venv/lib/python3.9/site-packages/pyspark/ml/tuning.py:858\u001B[0m, in \u001B[0;36mCrossValidator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    852\u001B[0m train \u001B[38;5;241m=\u001B[39m datasets[i][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcache()\n\u001B[1;32m    854\u001B[0m tasks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(\n\u001B[1;32m    855\u001B[0m     inheritable_thread_target(dataset\u001B[38;5;241m.\u001B[39msparkSession),\n\u001B[1;32m    856\u001B[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001B[1;32m    857\u001B[0m )\n\u001B[0;32m--> 858\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, metric, subModel \u001B[38;5;129;01min\u001B[39;00m pool\u001B[38;5;241m.\u001B[39mimap_unordered(\u001B[38;5;28;01mlambda\u001B[39;00m f: f(), tasks):\n\u001B[1;32m    859\u001B[0m     metrics_all[i][j] \u001B[38;5;241m=\u001B[39m metric\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m collectSubModelsParam:\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py:858\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    857\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 858\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    859\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    860\u001B[0m     item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_items\u001B[38;5;241m.\u001B[39mpopleft()\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:312\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 312\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 188
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Best ALS Model",
   "id": "76c25c9e4203c192"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "display(f\"Best Model Rank: {best_model.rank}\")\n",
    "display(f\"Best Model Params: {best_model.params}\")"
   ],
   "id": "cb5661476ab85094"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Get ALS Recommendations",
   "id": "db0c45b80e5c344d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T20:04:09.623609Z",
     "start_time": "2025-07-17T20:04:08.078175Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                                                                                                              |\n",
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|21220 |[{79921, 6.7020807}, {42789, 6.698961}, {86059, 6.668425}, {43595, 6.6322174}, {7816, 6.581626}, {79073, 6.560703}, {30237, 6.558498}, {42427, 6.556854}, {92458, 6.5219}, {81490, 6.520079}]|\n",
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 104,
   "source": [
    "# Get recommendations for user\n",
    "\n",
    "# Generate top 10 movie recommendations for a specified set of users\n",
    "# Pick user 21220 for now\n",
    "users = als_df_final_cached.filter(col(\"userId\") == 21220)\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "userSubsetRecs.show(truncate=False)\n",
    "\n",
    "rated_items = als_df_final_cached.filter(col(\"userId\") == 21220).select(\"productId\").rdd.flatMap(lambda x: x).collect()\n",
    "filtered_recs = [rec for rec in userSubsetRecs.collect()[0][\"recommendations\"] if rec.productId not in rated_items]\n",
    "filtered_recs_df = spark.createDataFrame(filtered_recs)\n",
    "\n",
    "als_df_final_cached_mapping = als_df_final_cached.select('productId', 'parent_asin', 'title').dropDuplicates()\n",
    "filtered_recs_df = filtered_recs_df.join(als_df_final_cached_mapping, 'productId', \"left\").sort(\"rating\", ascending = False)\n",
    "filtered_recs_df.show()"
   ],
   "id": "8d2d6064d39846c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T20:37:19.806850Z",
     "start_time": "2025-07-17T20:37:14.173958Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-----------+--------------------+\n",
      "|productId|            rating|parent_asin|               title|\n",
      "+---------+------------------+-----------+--------------------+\n",
      "|    79921| 6.702080726623535| B002XILY7K|BUNN REGFILTER 12...|\n",
      "|    42789| 6.698960781097412| B003EAHXO4|Whirlpool 3300163...|\n",
      "|    86059|   6.6684250831604| B00CWSZJD6|GE JT5500SFSS Ele...|\n",
      "|    43595|6.6322174072265625| B00WMUJ1LU|PS1485610 - Repla...|\n",
      "|     7816| 6.581625938415527| B0145SOZ5S|316557237 Range O...|\n",
      "|    79073| 6.560702800750732| B06X9QJFSF|GE APPLIANCE PART...|\n",
      "|    30237| 6.558497905731201| B00XN3DSBE|GENUINE Frigidair...|\n",
      "|    42427| 6.556853771209717| B000ZIMP9G|Whirlpool Part Nu...|\n",
      "|    92458| 6.521900177001953| B07L522PW2|Value 4 pack Kali...|\n",
      "|    81490| 6.520079135894775| B08C4VMP74|Ecumfy 4681EA2002...|\n",
      "+---------+------------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 130,
   "source": [
    "# Find already rated items and remove\n",
    "previously_rated_items = als_df_final_cached.filter(col(\"userId\") == 21220).select(\"productId\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Find recommendations that are unreviewed by the user\n",
    "ALS_filtered_recommendations = [rec for rec in userSubsetRecs.collect()[0][\"recommendations\"] if rec.productId not in previously_rated_items]\n",
    "ALS_filtered_recommendations_df = spark.createDataFrame(ALS_filtered_recommendations)\n",
    "\n",
    "# Add back meta data\n",
    "als_df_final_cached_mapping = als_df_final_cached.select('productId', 'parent_asin', 'title').dropDuplicates()\n",
    "als_df_final_cached_mapping_cached = als_df_final_cached_mapping.cache()\n",
    "ALS_filtered_recommendations_df = ALS_filtered_recommendations_df.join(als_df_final_cached_mapping_cached, 'productId', \"left\").sort(\"rating\", ascending = False)\n",
    "ALS_filtered_recommendations_df_cached = ALS_filtered_recommendations_df.cache()\n",
    "ALS_filtered_recommendations_df_cached.show()"
   ],
   "id": "d4869b3db23aa372"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T20:37:53.354080Z",
     "start_time": "2025-07-17T20:37:53.101711Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 132,
   "source": [
    "# Save recommendations locally\n",
    "ALS_filtered_recommendations_df_cached.write.parquet(r\"output/ALS_item_recs.parquet\", mode=\"overwrite\")"
   ],
   "id": "415eb0def447d6ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T01:05:28.038407Z",
     "start_time": "2025-07-18T01:04:56.639099Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "babc596268f6bc79",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/17 21:05:04 ERROR Instrumentation: org.apache.spark.SparkException: Job 126 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1301)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1299)\n",
      "\tat scala.collection.mutable.HashSet$Node.foreach(HashSet.scala:450)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:376)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1299)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3234)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:85)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:3120)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:3120)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2346)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2346)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2297)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:552)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1304)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:1106)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:753)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:730)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:632)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/07/17 21:05:09 ERROR Instrumentation: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.metrics$lzycompute(WholeStageCodegenExec.scala:647)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.metrics(WholeStageCodegenExec.scala:646)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$resetMetrics$2(SparkPlan.scala:148)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$resetMetrics$2$adapted(SparkPlan.scala:148)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:148)\n",
      "\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2220)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2219)\n",
      "\tat org.apache.spark.sql.classic.Dataset.rdd$lzycompute(Dataset.scala:1586)\n",
      "\tat org.apache.spark.sql.classic.Dataset.rdd(Dataset.scala:1584)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:743)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:730)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:632)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/07/17 21:05:09 ERROR Instrumentation: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.metrics$lzycompute(WholeStageCodegenExec.scala:647)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.metrics(WholeStageCodegenExec.scala:646)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$resetMetrics$2(SparkPlan.scala:148)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$resetMetrics$2$adapted(SparkPlan.scala:148)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:148)\n",
      "\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2220)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2219)\n",
      "\tat org.apache.spark.sql.classic.Dataset.rdd$lzycompute(Dataset.scala:1586)\n",
      "\tat org.apache.spark.sql.classic.Dataset.rdd(Dataset.scala:1584)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:743)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:730)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:632)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/07/17 21:05:10 ERROR Instrumentation: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.metrics$lzycompute(WholeStageCodegenExec.scala:647)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.metrics(WholeStageCodegenExec.scala:646)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$resetMetrics$2(SparkPlan.scala:148)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$resetMetrics$2$adapted(SparkPlan.scala:148)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:148)\n",
      "\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2220)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2219)\n",
      "\tat org.apache.spark.sql.classic.Dataset.rdd$lzycompute(Dataset.scala:1586)\n",
      "\tat org.apache.spark.sql.classic.Dataset.rdd(Dataset.scala:1584)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:743)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:730)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:632)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/07/17 21:05:10 ERROR Instrumentation: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.metrics$lzycompute(WholeStageCodegenExec.scala:647)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.metrics(WholeStageCodegenExec.scala:646)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$resetMetrics$2(SparkPlan.scala:148)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$resetMetrics$2$adapted(SparkPlan.scala:148)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:148)\n",
      "\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2220)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2219)\n",
      "\tat org.apache.spark.sql.classic.Dataset.rdd$lzycompute(Dataset.scala:1586)\n",
      "\tat org.apache.spark.sql.classic.Dataset.rdd(Dataset.scala:1584)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:743)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:730)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:632)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/07/17 21:05:19 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/9c/y4fn7g0j5fqfx7s_w4j5tdl40000gn/T/blockmgr-568102f0-3085-4356-a726-206a14b353ad/3b/temp_shuffle_1f0e00cb-497f-4061-aef0-1b983fb6d559\n",
      "25/07/17 21:05:19 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/9c/y4fn7g0j5fqfx7s_w4j5tdl40000gn/T/blockmgr-568102f0-3085-4356-a726-206a14b353ad/0c/temp_shuffle_1701c5c9-ba8b-4bf6-b4b8-9ad69f11b549\n",
      "25/07/17 21:05:19 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/9c/y4fn7g0j5fqfx7s_w4j5tdl40000gn/T/blockmgr-568102f0-3085-4356-a726-206a14b353ad/0c/temp_shuffle_9bced80c-d982-4c5a-af54-c35a24e1f3e7\n",
      "25/07/17 21:05:19 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/9c/y4fn7g0j5fqfx7s_w4j5tdl40000gn/T/blockmgr-568102f0-3085-4356-a726-206a14b353ad/18/temp_shuffle_e2560d0a-c41b-4e5f-93ad-632b33bb707b\n",
      "25/07/17 21:05:19 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/9c/y4fn7g0j5fqfx7s_w4j5tdl40000gn/T/blockmgr-568102f0-3085-4356-a726-206a14b353ad/1e/temp_shuffle_5ff04443-de2e-4e1d-b2a3-41134d47e217\n",
      "25/07/17 21:05:19 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/9c/y4fn7g0j5fqfx7s_w4j5tdl40000gn/T/blockmgr-568102f0-3085-4356-a726-206a14b353ad/25/temp_shuffle_1bfe7e5c-7c68-4b45-8691-0822778dd1e7\n",
      "25/07/17 21:05:19 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/9c/y4fn7g0j5fqfx7s_w4j5tdl40000gn/T/blockmgr-568102f0-3085-4356-a726-206a14b353ad/37/temp_shuffle_53b4abc6-899d-4330-af88-5fb37ffb31d0\n",
      "25/07/17 21:05:19 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/9c/y4fn7g0j5fqfx7s_w4j5tdl40000gn/T/blockmgr-568102f0-3085-4356-a726-206a14b353ad/2e/temp_shuffle_940b49f4-9957-48cd-b82d-d6424d992657\n",
      "25/07/17 21:05:19 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/9c/y4fn7g0j5fqfx7s_w4j5tdl40000gn/T/blockmgr-568102f0-3085-4356-a726-206a14b353ad/1f/temp_shuffle_ff9cdf81-d8fc-4013-9f49-598e3f0d7582\n",
      "25/07/17 21:05:19 ERROR Executor: Exception in task 9.0 in stage 1237.0 (TID 11486): /private/var/folders/9c/y4fn7g0j5fqfx7s_w4j5tdl40000gn/T/blockmgr-568102f0-3085-4356-a726-206a14b353ad/3b/temp_shuffle_1f0e00cb-497f-4061-aef0-1b983fb6d559 (No such file or directory)\n",
      "25/07/17 21:05:19 ERROR Executor: Exception in task 8.0 in stage 1237.0 (TID 11485): /private/var/folders/9c/y4fn7g0j5fqfx7s_w4j5tdl40000gn/T/blockmgr-568102f0-3085-4356-a726-206a14b353ad/0c/temp_shuffle_1701c5c9-ba8b-4bf6-b4b8-9ad69f11b549 (No such file or directory)\n"
     ]
    }
   ],
   "execution_count": 189
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simple Hybrid Model",
   "id": "14dca5776ed9390c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The purpose of this simple hybrid model is to use the recommendations from ALS (collaborative filtering) and rerank the items based off of the BERT similarity/distance score (content based recommender) to the top recommendation.",
   "id": "fdfa3f68809acb6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T02:54:30.252047Z",
     "start_time": "2025-07-18T02:54:30.082317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ALS_item_recommendations = pd.read_parquet(\"output/ALS_item_recs.parquet\")\n",
    "ALS_item_recommendations['rating'] = ALS_item_recommendations['rating'].clip(lower=0, upper=5)\n",
    "print(ALS_item_recommendations)\n",
    "# Find top item recommended\n",
    "top_rec_item = ALS_item_recommendations.head(1)['parent_asin'][0]"
   ],
   "id": "46aa449cbefd2776",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   productId  rating parent_asin                                              title\n",
      "0      79921    5.00  B002XILY7K  BUNN REGFILTER 12-Cup Regular Filters, Use wit...\n",
      "1      42789    5.00  B003EAHXO4                           Whirlpool 33001634 Timer\n",
      "2      86059    5.00  B00CWSZJD6            GE JT5500SFSS Electric Double Wall Oven\n",
      "3      43595    5.00  B00WMUJ1LU  PS1485610 - Replacement Washer Washing Machine...\n",
      "4       7816    5.00  B0145SOZ5S  316557237 Range Oven Control Board and Clock G...\n",
      "5      79073    5.00  B06X9QJFSF  GE APPLIANCE PARTS Smooth KIT WR12X22148 GE Ap...\n",
      "6      30237    5.00  B00XN3DSBE  GENUINE Frigidaire 241881318 Refrigerator Door...\n",
      "7      42427    5.00  B000ZIMP9G       Whirlpool Part Number JEA7000ADW: CARTRIDGE-\n",
      "8      92458    5.00  B07L522PW2  Value 4 pack Kalita Wave Filters, 185, Pack of...\n",
      "9      81490    5.00  B08C4VMP74  Ecumfy 4681EA2002H Drain Pump Compatible with ...\n"
     ]
    }
   ],
   "execution_count": 237
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T20:55:31.476863Z",
     "start_time": "2025-07-17T20:55:31.444576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Find BERT's recommend items for the top recommendation\n",
    "top_item_neighbors = predict_using_bert(top_rec_item)\n",
    "top_item_neighbors.head()"
   ],
   "id": "cf9eb53c687a5e10",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     neighbor_id                                     neighbor_title  distance\n",
       "2715  B0B8MFHWT2  279973 & 3392519 & 8577274 Dryer Thermal Cut-O...      0.25\n",
       "2716  B09TT3WN9H  279838 Heating Element for Kenmore Dryer Compa...      0.26\n",
       "2717  B078RF452M  MAYITOP Compatible Dryer Heating Element for S...      0.26\n",
       "2718  B082PJ9BWH  137114000 Dryer Heating Element Kit,137032600 ...      0.26\n",
       "2719  B0BWCTGXFT  (2023 Update) 3392519 Dryer Thermal Fuse Repla...      0.27"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbor_id</th>\n",
       "      <th>neighbor_title</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>B0B8MFHWT2</td>\n",
       "      <td>279973 &amp; 3392519 &amp; 8577274 Dryer Thermal Cut-O...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2716</th>\n",
       "      <td>B09TT3WN9H</td>\n",
       "      <td>279838 Heating Element for Kenmore Dryer Compa...</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>B078RF452M</td>\n",
       "      <td>MAYITOP Compatible Dryer Heating Element for S...</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>B082PJ9BWH</td>\n",
       "      <td>137114000 Dryer Heating Element Kit,137032600 ...</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2719</th>\n",
       "      <td>B0BWCTGXFT</td>\n",
       "      <td>(2023 Update) 3392519 Dryer Thermal Fuse Repla...</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T20:58:57.852734Z",
     "start_time": "2025-07-17T20:58:57.829354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_containing_ALS_item = ALS_item_recommendations['parent_asin'].isin(top_item_neighbors['neighbor_id'])\n",
    "\n",
    "print(\"Are any items in the BERT recommender?\")\n",
    "print(bert_containing_ALS_item)"
   ],
   "id": "384d908efcc9ae55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are any items in the BERT recommender?\n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "5    False\n",
      "6    False\n",
      "7    False\n",
      "8    False\n",
      "9    False\n",
      "Name: parent_asin, dtype: bool\n"
     ]
    }
   ],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T02:23:44.968296Z",
     "start_time": "2025-07-18T02:23:44.897711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Hold off until we have new data/larger KNN?\n",
    "#\n",
    "# # Need to match on parent_asin\n",
    "# top_item_neighbors = top_item_neighbors.rename(columns={'neighbor_id': 'parent_asin'})\n",
    "#\n",
    "# # Add in distance score and reranka\n",
    "# ALS_item_recs_BERT_rerank = ALS_item_recommendations.join(top_item_neighbors, 'parent_asin', \"left\").sort(\"distance\", ascending = False)"
   ],
   "id": "b9426eca91acea45",
   "outputs": [],
   "execution_count": 203
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Neural Network PyTorch?",
   "id": "8e2c2cdfdd9319dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T03:14:02.241200Z",
     "start_time": "2025-07-18T03:14:02.225305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from libreco.algorithms import NCF\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "be877e0e22ed0f23",
   "outputs": [],
   "execution_count": 241
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stl-recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
