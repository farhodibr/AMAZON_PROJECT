{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67aa21d",
   "metadata": {},
   "source": [
    "# DATA 612 FINAL PROJECT\n",
    "Amazon Product Recommender Model Using Reviews\n",
    "\n",
    "* Farhod Ibragimov\n",
    "* Gillian McGovern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481659ac",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Create an offline Amazon product (specifically Amazon appliances) recommender model using user ratings and reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3beadc",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "Source: https://amazon-reviews-2023.github.io/\n",
    "\n",
    "This is a large-scale Amazon Reviews dataset, collected in 2023 by McAuley Lab, and it includes rich features such as:\n",
    "\n",
    "*   User Reviews (ratings, text, helpfulness votes, etc.);\n",
    "*   Item Metadata (descriptions, price, raw image, etc.);\n",
    "*   Links (user-item / bought together graphs).\n",
    "\n",
    "\n",
    "User review structure can be found [here](https://amazon-reviews-2023.github.io/#for-user-reviews) and item metadata structure can be found [here](https://amazon-reviews-2023.github.io/#for-item-metadata).\n",
    "\n",
    "We will be specifically looking at the Appliances category of products, which includes:\n",
    "\n",
    "* 1.8M Users\n",
    "* 94.3K Appliances\n",
    "* 2.1M Ratings/Reviews\n",
    "\n",
    "The original data is in JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1f381f",
   "metadata": {},
   "source": [
    "## Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543af7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import torch\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql import SparkSession\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import floor, round, monotonically_increasing_id, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c312cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if False:\n",
    "    # CONFIG \n",
    "    REVIEW_PATH = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\Appliances.jsonl\"\n",
    "    META_PATH   = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\meta_Appliances.jsonl\"\n",
    "    OUT_DIR     = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\"\n",
    "    CHUNK_SIZE  = 200_000   # tune based on your machine's RAM\n",
    "    SEED        = 42\n",
    "\n",
    "\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    #LOAD METADATA \n",
    "    meta_pd = pd.read_json(\n",
    "        r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\meta_Appliances.jsonl\",\n",
    "        lines=True\n",
    "    )[[\"parent_asin\", \"average_rating\", \"rating_number\"]]\n",
    "\n",
    "    writers = {\"train\": None, \"valid\": None, \"test\": None}\n",
    "\n",
    "    # STREAM, FILTER, SPLIT, AND WRITE\n",
    "    for chunk in pd.read_json(\n",
    "        r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\Appliances.jsonl\",\n",
    "        lines=True,\n",
    "        chunksize=CHUNK_SIZE\n",
    "    ):\n",
    "        # 1) keep needed cols + timestamp\n",
    "        chunk = chunk[[\"user_id\", \"parent_asin\", \"rating\", \"timestamp\", \"text\"]]\n",
    "\n",
    "        # 2) filter to years 2021‚Äì2023\n",
    "        dt = pd.to_datetime(chunk[\"timestamp\"], unit=\"ms\")\n",
    "        mask_year = dt.dt.year.between(2021, 2023)\n",
    "        chunk = chunk.loc[mask_year]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        # 3) assign random float for splitting\n",
    "        rng = np.random.RandomState(SEED)\n",
    "        chunk[\"_rand\"] = rng.rand(len(chunk))\n",
    "\n",
    "        # 4) merge metadata\n",
    "        chunk = chunk.merge(meta_pd, on=\"parent_asin\", how=\"left\")\n",
    "\n",
    "        # 5) define split masks\n",
    "        masks = {\n",
    "            \"train\": chunk[\"_rand\"] <  0.8,\n",
    "            \"valid\": (chunk[\"_rand\"] >= 0.8) & (chunk[\"_rand\"] < 0.9),\n",
    "            \"test\":  chunk[\"_rand\"] >= 0.9\n",
    "        }\n",
    "\n",
    "        # 6) write each split to its Parquet\n",
    "        for split, m in masks.items():\n",
    "            sub = chunk.loc[m, [\n",
    "                \"user_id\",\n",
    "                \"parent_asin\",\n",
    "                \"rating\",\n",
    "                \"text\",\n",
    "                \"average_rating\",\n",
    "                \"rating_number\"\n",
    "            ]]\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            tbl = pa.Table.from_pandas(sub, preserve_index=False)\n",
    "            path = os.path.join(OUT_DIR, f\"{split}.parquet\")\n",
    "            if writers[split] is None:\n",
    "                writers[split] = pq.ParquetWriter(path, schema=tbl.schema)\n",
    "            writers[split].write_table(tbl)\n",
    "\n",
    "    # close Parquet writers\n",
    "    for w in writers.values():\n",
    "        if w:\n",
    "            w.close()\n",
    "\n",
    "    print(\"Finished writing filtered splits to:\", OUT_DIR)\n",
    "    print(\"  ‚Ä¢\", os.path.join(OUT_DIR, \"train.parquet\"))\n",
    "    print(\"  ‚Ä¢\", os.path.join(OUT_DIR, \"valid.parquet\"))\n",
    "    print(\"  ‚Ä¢\", os.path.join(OUT_DIR, \"test.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa20bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:    \n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    for split in (\"train\", \"valid\", \"test\"):\n",
    "        path = rf\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\{split}.parquet\"\n",
    "        pf = pq.ParquetFile(path)\n",
    "        print(f\"{split.capitalize()} split: {pf.metadata.num_rows} reviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b113b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "if False:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"CheckRatingRange\") \\\n",
    "        .config(\"spark.driver.memory\",\"4g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Load just the rating column from the train split\n",
    "    train = spark.read.parquet(\n",
    "        r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\train.parquet\"\n",
    "    ).select(\"rating\")\n",
    "\n",
    "    # 1) min & max\n",
    "    train.selectExpr(\"min(rating) AS min_rating\", \"max(rating) AS max_rating\") \\\n",
    "        .show()\n",
    "\n",
    "    # 2) basic summary (mean, stddev, etc.)\n",
    "    train.describe(\"rating\").show()\n",
    "\n",
    "    # 3) breakdown by rating value\n",
    "    train.groupBy(\"rating\").count().orderBy(\"rating\").show(5, truncate=False)\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651ded8",
   "metadata": {},
   "source": [
    "## BERT Content Based Recommender Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73a532",
   "metadata": {},
   "source": [
    "### Create BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4680b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN THIS CELL UNLESS EMBEDINGS ARE NEEDED\n",
    "if False:\n",
    "    import os\n",
    "    import torch\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    from pyspark.sql import SparkSession\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from huggingface_hub import hf_hub_download\n",
    "\n",
    "    HF_REPO      = \"glavvrach79/my-recsys-data\"\n",
    "    HF_SUBFOLDER = \"output/embeddings\"\n",
    "    FNAME        = \"item_embeddings.parquet\"\n",
    "\n",
    "    LOCAL_TRAIN  = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\train.parquet\"\n",
    "    EMB_OUT      = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\embeddings\\item_embeddings.parquet\"\n",
    "\n",
    "    DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SAMPLE_PER   = 5\n",
    "    BATCH_SIZE   = 64\n",
    "\n",
    "    os.makedirs(os.path.dirname(EMB_OUT), exist_ok=True)\n",
    "    print(f\"Using device: {DEVICE}\\n\")\n",
    "\n",
    "    def is_good_parquet(path: str) -> bool:\n",
    "        \"\"\"Try a light PyArrow read to confirm the file is a valid Parquet.\"\"\"\n",
    "        try:\n",
    "            pq.read_table(path, columns=[])  # only footer\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    downloaded = False\n",
    "\n",
    "    # if a local copy already exists & is valid, use it \n",
    "    if os.path.exists(EMB_OUT) and is_good_parquet(EMB_OUT):\n",
    "        print(f\"Found valid local embeddings ‚Üí {EMB_OUT}\\n\")\n",
    "        downloaded = True\n",
    "\n",
    "    # else try downloading from HF \n",
    "    if not downloaded:\n",
    "        try:\n",
    "            hf_path = hf_hub_download(\n",
    "                repo_id=HF_REPO,\n",
    "                repo_type=\"dataset\",\n",
    "                subfolder=HF_SUBFOLDER,\n",
    "                filename=FNAME,\n",
    "            )\n",
    "            print(f\"üì¶ Downloaded HF embeddings ‚Üí {hf_path}\")\n",
    "            if is_good_parquet(hf_path):\n",
    "                print(\"HF parquet loads OK. Using HF version.\\n\")\n",
    "                EMB_OUT = hf_path\n",
    "                downloaded = True\n",
    "            else:\n",
    "                print(\"HF parquet is corrupt‚Äîwill recompute locally.\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch from HF ({e!r}); will compute locally.\\n\")\n",
    "\n",
    "    # if still not obtained, recompute locally \n",
    "    if not downloaded:\n",
    "        print(\"‚ñ∂Ô∏è Computing embeddings locally‚Ä¶\\n\")\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"ComputeItemEmbeddings\") \\\n",
    "            .config(\"spark.driver.memory\",\"16g\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        df = spark.read.parquet(LOCAL_TRAIN).select(\"parent_asin\", \"text\")\n",
    "        pdf = df.toPandas()\n",
    "        spark.stop()\n",
    "\n",
    "        model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=DEVICE)\n",
    "        writer = None\n",
    "\n",
    "        for pid, group in pdf.groupby(\"parent_asin\", sort=False):\n",
    "            samples = group[\"text\"].sample(\n",
    "                n=min(len(group), SAMPLE_PER),\n",
    "                random_state=42\n",
    "            ).tolist()\n",
    "\n",
    "            emb_batch = model.encode(\n",
    "                samples,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                show_progress_bar=False,\n",
    "                convert_to_numpy=True\n",
    "            )\n",
    "            mean_emb = emb_batch.mean(axis=0)\n",
    "\n",
    "            data = {\"parent_asin\": [pid]}\n",
    "            for i, val in enumerate(mean_emb):\n",
    "                data[f\"emb_{i}\"] = [float(val)]\n",
    "            table = pa.Table.from_pydict(data)\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(EMB_OUT, schema=table.schema)\n",
    "            writer.write_table(table)\n",
    "\n",
    "        if writer:\n",
    "            writer.close()\n",
    "\n",
    "        print(f\"‚úÖ Wrote item embeddings locally ‚Üí {EMB_OUT}\\n\")\n",
    "\n",
    "    # preview final \n",
    "    print(f\" Final embeddings path ‚Üí {EMB_OUT}\\n\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PreviewEmbeddings\") \\\n",
    "        .config(\"spark.driver.memory\",\"4g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    def try_spark_read(path: str):\n",
    "        try:\n",
    "            return spark.read.parquet(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Spark read failed on `{path}`:\\n   {e}\")\n",
    "            return None\n",
    "\n",
    "    emb_df = try_spark_read(EMB_OUT)\n",
    "\n",
    "    # ‚îÄ‚îÄ If Spark failed on the HF‚Äêcached file, fall back to pandas ‚Üí Spark ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    if emb_df is None:\n",
    "        import pandas as pd\n",
    "        print(\"Falling back to pandas.read_parquet ‚Ä¶\")\n",
    "        pdf = pd.read_parquet(EMB_OUT)\n",
    "        emb_df = spark.createDataFrame(pdf)\n",
    "\n",
    "    emb_df.show(5, truncate=50)\n",
    "    print(\"Total items:\", emb_df.count())\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8529d4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded KNN cache from HF: C:\\Users\\farho\\.cache\\huggingface\\hub\\datasets--glavvrach79--my-recsys-data\\snapshots\\216e868cbbb098622e859a3b1275a215acc20122\\output\\knn_cache.parquet\n",
      "Loaded cache from HF!\n"
     ]
    }
   ],
   "source": [
    "# CONFIG \n",
    "HF_REPO      = \"glavvrach79/my-recsys-data\"\n",
    "HF_SUBFOLDER_KNN = \"output\"\n",
    "HF_SUBFOLDER = \"output/embeddings\"\n",
    "FNAME        = \"item_embeddings.parquet\"\n",
    "TRAIN_NAME   = \"train.parquet\"\n",
    "TEST_NAME  = \"test.parquet\"\n",
    "VALID_NAME = \"valid.parquet\"\n",
    "FULL_REVIEW_NAME = \"full_review.parquet\"\n",
    "KNN_NAME = \"knn_cache.parquet\"\n",
    "\n",
    "META_PATH    = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\meta_Appliances.jsonl\"\n",
    "TOP_K        = 5\n",
    "OUT_PATH     = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\knn_cache.parquet\"\n",
    "\n",
    "#os.makedirs(os.path.dirname(LOCAL_OUT), exist_ok=True)\n",
    "\n",
    "def load_or_build_cache():\n",
    "    # try HF download + read\n",
    "    try:\n",
    "        hf_cache = hf_hub_download(\n",
    "            repo_id=HF_REPO,\n",
    "            repo_type=\"dataset\",\n",
    "            subfolder=HF_SUBFOLDER_KNN,\n",
    "            filename=KNN_NAME,\n",
    "        )\n",
    "        print(\"Downloaded KNN cache from HF:\", hf_cache)\n",
    "        cache = pd.read_parquet(hf_cache)\n",
    "        print(\"Loaded cache from HF!\")\n",
    "        return cache\n",
    "    except Exception as e:\n",
    "        print(f\"HF cache unavailable ({e}); building locally‚Ä¶\")\n",
    "\n",
    "    \n",
    "    # download embeddings\n",
    "    hf_emb = hf_hub_download(\n",
    "        repo_id=HF_REPO,\n",
    "        repo_type=\"dataset\",\n",
    "        subfolder=HF_SUBFOLDER,\n",
    "        filename=FNAME,\n",
    "    )\n",
    "    df_emb = pd.read_parquet(hf_emb)\n",
    "\n",
    "    # metadata\n",
    "    df_meta = (\n",
    "        pd.read_json(META_PATH, lines=True)\n",
    "          .loc[:, [\"parent_asin\", \"title\"]]\n",
    "          .rename(columns={\"parent_asin\":\"item_id\", \"title\":\"product_title\"})\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        df_emb.rename(columns={\"parent_asin\":\"item_id\"})\n",
    "              .merge(df_meta, on=\"item_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    X        = df.filter(regex=\"^emb_\").values\n",
    "    item_ids = df[\"item_id\"].to_numpy()\n",
    "    titles   = df[\"product_title\"].to_numpy()\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=TOP_K+1, metric=\"cosine\")\n",
    "    nn.fit(X)\n",
    "    distances, indices = nn.kneighbors(X)\n",
    "\n",
    "    title_map = df.set_index(\"item_id\")[\"product_title\"]\n",
    "\n",
    "    cache = pd.DataFrame({\n",
    "        \"item_id\":        np.repeat(item_ids, TOP_K),\n",
    "        \"item_title\":     np.repeat(title_map[item_ids].values, TOP_K),      # ‚Üê add this\n",
    "        \"rank\":           np.tile(np.arange(1, TOP_K+1), len(item_ids)),\n",
    "        \"neighbor_id\":    item_ids[indices[:, 1:]].ravel(),\n",
    "        \"neighbor_title\": titles[indices[:, 1:]].ravel(),\n",
    "        \"distance\":       distances[:, 1:].ravel(),\n",
    "    })\n",
    "    cache.to_parquet(OUT_PATH, index=False)\n",
    "    print(f\"file saved to {OUT_PATH}\")\n",
    "    \n",
    "    return cache\n",
    "\n",
    "\n",
    "knn_cache = load_or_build_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189dd02",
   "metadata": {},
   "source": [
    "### Create Content Based Model Using BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0d8a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Randomly selected: B07BTQKG5J ‚Üí [UPGRADED] DC47-00019A Samsung Dryer Heating Element & DC96-00887A Thermal Fuse & DC47-00018A Thermostat COMPLETE Dryer Repair Kit Replacement by BlueStars - Exact Fit For Samsung & Kenmore Dryers\n",
      "\n",
      "Top similar items:\n",
      "neighbor_id                                                                                                                                                                            neighbor_title  distance\n",
      " B0B8MFHWT2    279973 & 3392519 & 8577274 Dryer Thermal Cut-Off Fuse Kit with Thermistor Control and Thermal Fuse by Blutoget -Compatible for Whirlpool, Ken-more, Sam-sung, May-tag electric Dryers.  0.248161\n",
      " B09TT3WN9H                                                279838 Heating Element for Kenmore Dryer Compatible Whirlpool Wed4815ew1 Heating Elements Roper Dryer Parts 500 600 70 80 Series Model 110  0.259680\n",
      " B078RF452M                      MAYITOP Compatible Dryer Heating Element for Samsung DV42H5200EP/A3-0000, Samsung DV361EWBEWR/A3-0001, Samsung DV219AEB/XAA-0000, Samsung DV393ETPARA/A1-0001 Dryers  0.262173\n",
      " B082PJ9BWH                                             137114000 Dryer Heating Element Kit,137032600 Thermal Limiter and High Limit Thermostat 3204267 Combo Set,Replacement for AP4456656 PS2367792  0.263069\n",
      " B0BWCTGXFT (2023 Update) 3392519 Dryer Thermal Fuse Replacement Part by BlueStars - Kenmore Dryer Thermal Fuse Exact Fit for Whirlpool Kenmore - Replaces AP6008325 3388651 694511 80005 WP3392519VP  0.272632\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "cache = knn_cache\n",
    "\n",
    "# load the meta table so we can lookup item titles\n",
    "meta = (\n",
    "    pd.read_json(META_PATH, lines=True)\n",
    "      .rename(columns={\"parent_asin\":\"item_id\", \"title\":\"product_title\"})\n",
    "      .set_index(\"item_id\")[\"product_title\"]\n",
    ")\n",
    "\n",
    "# pick a random item_id\n",
    "random_id = cache[\"item_id\"].drop_duplicates().sample(1, random_state=42).iloc[0]\n",
    "random_title = meta[random_id]\n",
    "\n",
    "# pull its top‚ÄëK neighbors from the cache\n",
    "neighbors = (\n",
    "    cache[cache[\"item_id\"] == random_id]\n",
    "      .sort_values(\"rank\")\n",
    "      .loc[:, [\"neighbor_id\",\"neighbor_title\",\"distance\"]]\n",
    ")\n",
    "\n",
    "print(f\"üé≤ Randomly selected: {random_id} ‚Üí {random_title}\\n\")\n",
    "print(\"Top similar items:\")\n",
    "print(neighbors.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca8b0ec",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Spark Model (ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613aa08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      "\n",
      "+--------------------+-----------+------+\n",
      "|             user_id|parent_asin|rating|\n",
      "+--------------------+-----------+------+\n",
      "|AGKHLEW2SOWHNMFQI...| B01N0TQ0OH|     5|\n",
      "|AHWWLSPCJMALVHDDV...| B07DD37QPZ|     5|\n",
      "|AHZIJGKEWRTAEOZ67...| B082W3Z9YK|     5|\n",
      "|AFGUPTDFAWOHHL4LZ...| B078W2BJY8|     5|\n",
      "|AELFJFAXQERUSMTXJ...| B08C9LPCQV|     5|\n",
      "+--------------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ALSPrep\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# download the reviews parquet from HF\n",
    "hf_path = hf_hub_download(\n",
    "    repo_id=\"glavvrach79/my-recsys-data\",\n",
    "    repo_type=\"dataset\",\n",
    "    filename=\"full_review.parquet\",\n",
    ")\n",
    "\n",
    "# read it directly with spark.read.parquet\n",
    "als_full_df = spark.read.parquet(hf_path).cache()\n",
    "\n",
    "#sanity‚Äêcheck the schema and small sample\n",
    "als_full_df.printSchema()\n",
    "als_full_df.limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f9093e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------+---------+------------+\n",
      "|parent_asin|             user_id|rating|userIntId|productIntId|\n",
      "+-----------+--------------------+------+---------+------------+\n",
      "| B0053F9NY6|AE5AWKLOJHY2E5HEW...|     5|    12645|       27016|\n",
      "| B0C6FDT9L2|AEIMWAEJUVTMJRYIQ...|     1|    87136|       55207|\n",
      "| B00Q4X2GDQ|AEVEMRP62HZ7JWQ3N...|     5|    36587|       55622|\n",
      "| B00Q4X2FSM|AEVEMRP62HZ7JWQ3N...|     5|    36587|       51534|\n",
      "| B07GRB1BKD|AEXOHXABEMAGG225Y...|     5|   135632|       11256|\n",
      "+-----------+--------------------+------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distinct users + assign stable integer IDs\n",
    "users = (als_full_df\n",
    "         .select(\"user_id\")\n",
    "         .distinct()\n",
    "         .coalesce(1)\n",
    "         .withColumn(\"userIntId\", monotonically_increasing_id())\n",
    "         .cache())\n",
    "\n",
    "# distinct products + assign stable integer IDs\n",
    "products = (als_full_df\n",
    "            .select(\"parent_asin\")\n",
    "            .distinct()\n",
    "            .coalesce(1)\n",
    "            .withColumn(\"productIntId\", monotonically_increasing_id())\n",
    "            .cache())\n",
    "\n",
    "# join them back\n",
    "als_df_int_ids = (als_full_df\n",
    "                  .join(users,    \"user_id\",     \"left\")\n",
    "                  .join(products, \"parent_asin\", \"left\"))\n",
    "\n",
    "# show a small sample\n",
    "als_df_int_ids.limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "492ec6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the ratings into training and test data\n",
    "als_df_final = als_df_int_ids.select(col(\"userIntId\").alias(\"userId\"), col(\"productIntId\").alias(\"productId\"), col(\"rating\"), col('parent_asin'))\n",
    "als_df_final_cached = als_df_final.cache()\n",
    "\n",
    "(training_data, test_data) = als_df_final_cached.randomSplit([0.7, 0.3], seed=42)\n",
    "test_data_cached = test_data.cache()\n",
    "training_data_cached = training_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca3210e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+----------+\n",
      "|userId|productId|rating|prediction|\n",
      "+------+---------+------+----------+\n",
      "|28    |4748     |4     |3.6311698 |\n",
      "|28    |5684     |5     |3.3890977 |\n",
      "|28    |70604    |4     |1.3954598 |\n",
      "|148   |3331     |5     |3.846614  |\n",
      "|148   |21264    |5     |3.665117  |\n",
      "|155   |70608    |1     |3.215338  |\n",
      "|183   |53764    |4     |2.090805  |\n",
      "|211   |10331    |5     |3.868464  |\n",
      "|385   |36806    |4     |3.0781858 |\n",
      "|496   |139      |4     |1.5207388 |\n",
      "+------+---------+------+----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Create model without any hyperparameter tuning\n",
    "\n",
    "# Set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(userCol=\"userId\", itemCol=\"productId\", ratingCol=\"rating\", rank = 10, maxIter = 15, regParam = .1,\n",
    "          coldStartStrategy=\"drop\", nonnegative = True, implicitPrefs = False)\n",
    "\n",
    "# Fit the model to the training_data\n",
    "model = als.fit(training_data_cached)\n",
    "\n",
    "# Generate predictions on the test_data\n",
    "test_predictions = model.transform(test_data_cached)\n",
    "preds = test_predictions.select(\"userId\",\"productId\",\"rating\",\"prediction\")\n",
    "\n",
    "# preview only the first 10 rows\n",
    "preds.show(10, truncate=False)\n",
    "\n",
    "# now it‚Äôs safe to shut Spark down\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfcf326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: hyperparameter tuning\n",
    "\n",
    "# # Hyperparameter Tuning\n",
    "#\n",
    "# # Use pyspark grid search\n",
    "# param_grid = ParamGridBuilder() \\\n",
    "#            .addGrid(als.rank, [10, 50, 75, 100]) \\\n",
    "#            .addGrid(als.maxIter, [10]) \\\n",
    "#            .addGrid(als.regParam, [.05, .1, .15]) \\\n",
    "#            .build()\n",
    "#\n",
    "# # Create RMSE evaluator\n",
    "# evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "#\n",
    "# # Use cross validation\n",
    "# cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5, collectSubModels=True)\n",
    "#\n",
    "# # Checkpoint the training data to truncate its lineage.\n",
    "# # This is a lazy operation, it will be triggered by the .fit() call.\n",
    "# training_data_chkp = training_data_cached.checkpoint()\n",
    "#\n",
    "# # Fit the cross validator on the CHECKPOINTED DataFrame.\n",
    "# model = cv.fit(training_data_chkp)\n",
    "#\n",
    "# # Best model\n",
    "# best_model = model.bestModel\n",
    "#\n",
    "# # Average RMSE for each model\n",
    "# avg_rmse_models = model.avgMetrics\n",
    "#\n",
    "# display(f\"{len(param_grid)} models tested\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stl-recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
