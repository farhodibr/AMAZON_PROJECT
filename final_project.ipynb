{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c312cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing filtered splits to: C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\n",
      "  • C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\train.parquet\n",
      "  • C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\valid.parquet\n",
      "  • C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\test.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# CONFIG \n",
    "REVIEW_PATH = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\Appliances.jsonl\"\n",
    "META_PATH   = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\meta_Appliances.jsonl\"\n",
    "OUT_DIR     = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\"\n",
    "CHUNK_SIZE  = 200_000   # tune based on your machine's RAM\n",
    "SEED        = 42\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "#LOAD METADATA \n",
    "meta_pd = pd.read_json(\n",
    "    r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\meta_Appliances.jsonl\",\n",
    "    lines=True\n",
    ")[[\"parent_asin\", \"average_rating\", \"rating_number\"]]\n",
    "\n",
    "writers = {\"train\": None, \"valid\": None, \"test\": None}\n",
    "\n",
    "# STREAM, FILTER, SPLIT, AND WRITE\n",
    "for chunk in pd.read_json(\n",
    "    r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\Appliances.jsonl\",\n",
    "    lines=True,\n",
    "    chunksize=CHUNK_SIZE\n",
    "):\n",
    "    # 1) keep needed cols + timestamp\n",
    "    chunk = chunk[[\"user_id\", \"parent_asin\", \"rating\", \"timestamp\", \"text\"]]\n",
    "\n",
    "    # 2) filter to years 2021–2023\n",
    "    dt = pd.to_datetime(chunk[\"timestamp\"], unit=\"ms\")\n",
    "    mask_year = dt.dt.year.between(2021, 2023)\n",
    "    chunk = chunk.loc[mask_year]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # 3) assign random float for splitting\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    chunk[\"_rand\"] = rng.rand(len(chunk))\n",
    "\n",
    "    # 4) merge metadata\n",
    "    chunk = chunk.merge(meta_pd, on=\"parent_asin\", how=\"left\")\n",
    "\n",
    "    # 5) define split masks\n",
    "    masks = {\n",
    "        \"train\": chunk[\"_rand\"] <  0.8,\n",
    "        \"valid\": (chunk[\"_rand\"] >= 0.8) & (chunk[\"_rand\"] < 0.9),\n",
    "        \"test\":  chunk[\"_rand\"] >= 0.9\n",
    "    }\n",
    "\n",
    "    # 6) write each split to its Parquet\n",
    "    for split, m in masks.items():\n",
    "        sub = chunk.loc[m, [\n",
    "            \"user_id\",\n",
    "            \"parent_asin\",\n",
    "            \"rating\",\n",
    "            \"text\",\n",
    "            \"average_rating\",\n",
    "            \"rating_number\"\n",
    "        ]]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        tbl = pa.Table.from_pandas(sub, preserve_index=False)\n",
    "        path = os.path.join(OUT_DIR, f\"{split}.parquet\")\n",
    "        if writers[split] is None:\n",
    "            writers[split] = pq.ParquetWriter(path, schema=tbl.schema)\n",
    "        writers[split].write_table(tbl)\n",
    "\n",
    "# close Parquet writers\n",
    "for w in writers.values():\n",
    "    if w:\n",
    "        w.close()\n",
    "\n",
    "print(\"Finished writing filtered splits to:\", OUT_DIR)\n",
    "print(\"  •\", os.path.join(OUT_DIR, \"train.parquet\"))\n",
    "print(\"  •\", os.path.join(OUT_DIR, \"valid.parquet\"))\n",
    "print(\"  •\", os.path.join(OUT_DIR, \"test.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa20bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split: 616463 reviews\n",
      "Valid split: 77195 reviews\n",
      "Test split: 76002 reviews\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "for split in (\"train\", \"valid\", \"test\"):\n",
    "    path = rf\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\{split}.parquet\"\n",
    "    pf = pq.ParquetFile(path)\n",
    "    print(f\"{split.capitalize()} split: {pf.metadata.num_rows} reviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b113b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CheckRatingRange\") \\\n",
    "    .config(\"spark.driver.memory\",\"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load just the rating column from the train split\n",
    "train = spark.read.parquet(\n",
    "    r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\train.parquet\"\n",
    ").select(\"rating\")\n",
    "\n",
    "# 1) min & max\n",
    "train.selectExpr(\"min(rating) AS min_rating\", \"max(rating) AS max_rating\") \\\n",
    "     .show()\n",
    "\n",
    "# 2) basic summary (mean, stddev, etc.)\n",
    "train.describe(\"rating\").show()\n",
    "\n",
    "# 3) breakdown by rating value\n",
    "train.groupBy(\"rating\").count().orderBy(\"rating\").show(5, truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4680b3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Wrote item embeddings to: C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\embeddings\\item_embeddings.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyspark.sql import SparkSession\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "TRAIN_PARQUET = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\train.parquet\"\n",
    "EMB_OUT       = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\embeddings\\item_embeddings.parquet\"\n",
    "SAMPLE_PER    = 5        \n",
    "BATCH_SIZE    = 64      \n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "os.makedirs(os.path.dirname(EMB_OUT), exist_ok=True)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ItemBERTEmbeddings\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(TRAIN_PARQUET).select(\"parent_asin\", \"text\")\n",
    "pdf = df.toPandas()   \n",
    "spark.stop()\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=DEVICE)\n",
    "\n",
    "writer = None\n",
    "schema = None\n",
    "\n",
    "for pid, group in pdf.groupby(\"parent_asin\", sort=False):\n",
    "    texts = group[\"text\"].sample(\n",
    "        n=min(len(group), SAMPLE_PER),\n",
    "        random_state=42\n",
    "    ).tolist()\n",
    "\n",
    "    embs = model.encode(\n",
    "        texts,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        show_progress_bar=False,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    mean_emb = embs.mean(axis=0)\n",
    "\n",
    "    data = {\"parent_asin\": [pid]}\n",
    "    for i, v in enumerate(mean_emb):\n",
    "        data[f\"emb_{i}\"] = [float(v)]\n",
    "    table = pa.Table.from_pydict(data)\n",
    "\n",
    "    if writer is None:\n",
    "        schema = table.schema\n",
    "        writer = pq.ParquetWriter(EMB_OUT, schema=schema)\n",
    "\n",
    "\n",
    "    writer.write_table(table)\n",
    "\n",
    "\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(\"Wrote item embeddings to:\", EMB_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8529d4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample item:\n",
      " - ID:    B07DD37QPZ\n",
      " - Title: Essential Values 18 Pack Compatible Replacement Filters (90 Dryer Loads Total) for Bettervent Indoor Dryer Vent\n",
      "\n",
      "Top 5 similar items by title:\n",
      "1. Polyester Lint Trap Dryer Filter Replacement Part Compatible With Bettervent Indoor Electric Dryers Vent Kit Pack of 12\n",
      "2. 97006931 Range Hood Aluminum Filter for Compatible with Kenmore Broan S97006931 1172137 5-3078 88150 99010121 C88150 K758900 Range Hood Filter BP29 (2 pack)\n",
      "3. VBENLEM Pack of 6 Hood Filters 19.5W x 15.5H Inch, 430 Stainless Steel 5 Grooves Commercial Hood Filters, Range Hood Filter for Grease Rated Commercial Kitchen Exhaust Hoods\n",
      "4. Refrigerator Air Filter Replacement for LG LT120F Kenmore Elite 46-9918, 469918, 9918, Replace ADQ73214402, ADQ73214403, ADQ73214404, ADQ73334008-6 Pack\n",
      "5. Colorfullife 6 Pack Humidifier Wicking Filter T for Honeywell Top Fill Tower Humidifier HEV615, HEV620, Replacement Filter T, Replace Parts HFT600T HFT600PDQ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# ─── CONFIG ──────────────────────────────────────────────────────────────────\n",
    "EMB_PATH   = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\output\\embeddings\\item_embeddings.parquet\"\n",
    "META_PATH  = r\"C:\\CUNY_MSDS\\DATA612\\AMAZON_PROJECT\\data\\meta_Appliances.jsonl\"\n",
    "TOP_K      = 5\n",
    "\n",
    "# ─── 1) Load embeddings and metadata ─────────────────────────────────────────\n",
    "df_emb = pd.read_parquet(EMB_PATH)\n",
    "df_meta = pd.read_json(META_PATH, lines=True)[[\"parent_asin\", \"title\"]]\n",
    "df_meta = df_meta.rename(columns={\"parent_asin\": \"item_id\", \"title\": \"product_title\"})\n",
    "\n",
    "# ─── 2) Merge to get titles alongside embeddings ─────────────────────────────\n",
    "df = df_emb.rename(columns={\"parent_asin\": \"item_id\"}).merge(df_meta, on=\"item_id\", how=\"left\")\n",
    "\n",
    "# ─── 3) Fit Nearest Neighbors on embedding vectors ────────────────────────────\n",
    "X = df.filter(regex=\"^emb_\").values\n",
    "item_ids = df[\"item_id\"].values\n",
    "titles   = df[\"product_title\"].values\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=TOP_K+1, metric=\"cosine\")\n",
    "nn.fit(X)\n",
    "\n",
    "# ─── 4) Recommendation function returning titles ─────────────────────────────\n",
    "def recommend_titles(item_id: str, top_k: int = TOP_K):\n",
    "    if item_id not in item_ids:\n",
    "        raise ValueError(f\"Item ID {item_id} not found.\")\n",
    "    idx = np.where(item_ids == item_id)[0][0]\n",
    "    distances, indices = nn.kneighbors([X[idx]], n_neighbors=top_k+1)\n",
    "    rec_idxs = indices[0][1:]\n",
    "    return titles[rec_idxs].tolist()\n",
    "\n",
    "# ─── 5) Show for a sample item ───────────────────────────────────────────────\n",
    "sample_id = item_ids[:5]\n",
    "print(\"Sample item:\")\n",
    "print(\" - ID:   \", sample_id)\n",
    "print(\" - Title:\", titles[0])\n",
    "print(\"\\nTop 5 similar items by title:\")\n",
    "for rank, pt in enumerate(recommend_titles(sample_id), start=1):\n",
    "    print(f\"{rank}. {pt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stl-recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
